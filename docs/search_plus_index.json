{"./":{"url":"./","title":"Introduction","keywords":"","body":"bigdata-introduction bigdata introduction Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-11-25 11:48:36 "},"spark/spark_quick_start.html":{"url":"spark/spark_quick_start.html","title":"Spark Quick Start","keywords":"","body":"Spark Quick Start 一、安装 spark下载&解压 Quick start Spark Overview Spark Programming Guide Running Spark on YARN 二、scala shell ./bin/spark-shell scala> val lines = sc.textFile(\"/Users/huanghaifeng/Documents/study/spark/derby.log\") lines: org.apache.spark.rdd.RDD[String] = /Users/huanghaifeng/Documents/spark/derby.log MapPartitionsRDD[1] at textFile at :27 scala> lines.count() res4: Long = 13 scala> lines.first() scala> val sparkLine = lines.filter(line => line.contains(\"spark\")) sparkLine: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at :29 scala> sparkLine.first() res7: String = \"on database directory /Users/huanghaifeng/Documents/spark/metastore_db with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@6bc8d8bd \" scala> sparkLine.count() res8: Long = 3 三、核心概念介绍 3.1 宏观overview Speed Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk. Ease of Use Write applications quickly in Java, Scala, Python, R. Generality Combine SQL, streaming, and complex analytics. Runs Everywhere Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3. 引用源自Spark官网 3.2 spark知识点 Spark Core Api Spark SQL Spark Streaming 流式计算 Spark MLib 机器学习 Spark GraphX 并行的图计算 Spark的集群管理器（YARN、Mesos、自带的独立调度器） 3.3 任务运行过程 一个Spark应用 --> 一个驱动器(driver program)节点 --> 多个工作节点(worker node)一个工作节点 --> 一个执行器(executor) --> 多个并行任务(task) 3.4 几个任务概念的区分 http://spark.apache.org/docs/latest/cluster-overview.html job 一系列stage组成一个job，一个行动就是一个job stage 一个job可以分为多个stage， stage划分的条件，shuffle或者行动操作 task executor上的最小任务单元称之为task，task是并行的，单个shuffle根据partition数划分成n个tasks Term Meaning Application User program built on Spark. Consists of a driver program and executors on the cluster. Application jar A jar containing the user's Spark application. In some cases users will want to create an \"uber jar\" containing their application along with its dependencies. The user's jar should never include Hadoop or Spark libraries, however, these will be added at runtime. Driver program The process running the main() function of the application and creating the SparkContext Cluster manager An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN) Deploy mode Distinguishes where the driver process runs. In \"cluster\" mode, the framework launches the driver inside of the cluster. In \"client\" mode, the submitter launches the driver outside of the cluster. Worker node Any node that can run application code in the cluster Executor A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. Task A unit of work that will be sent to one executor Job A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you'll see this term used in the driver's logs. Stage Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you'll see this term used in the driver's logs. 四、Spark Core原理解析 4.1 架构 Spark集群采用的是典型的主 / 从结构 一个Spark应用(application) = 一个中央协调驱动器(Driver)节点 + 多个执行任务的执行器(Executor)节点； 【Executor】负责并行的执行任务（task）、存储必要的RDD数据 【Driver】负责任务拆分、任务调度 程序之间的RDD变换关系组成了一张逻辑上的DAG，程序运行时逻辑图将转换为物理执行过程，Driver在对任务划分的时候会将连续的映射转为流水线，将多个操作合并到同一个步骤（stage）中来，明显的例子 val local_lines = sc.textFile(\"XXX\") local_lines.first() #连续起来执行后只需要加载文件的第一行 Application -> Jobs -> stages -> tasks 1) val local_lines = sc.textFile(\"XXX\") 1.2) val local_lines_1 = local_lines.map(xxx) 2) val local_lines_2 = sc.textFile(\"XXX\") 3) println(local_lines_1.union(local_lines_2)) # 3)是一个job，可以拆分为1*）、2）两个stage # 每个stage可以分为并行的多个task 【集群管理器】启动执行器节点，某些特定情况(比如、--deloy-mode=cluster)下才会靠集群管理器来启动驱动器节点。程序启动的时候，驱动器程序与集群管理器通信申请资源启动执行器节点；程序结束的时候，驱动器程序终止执行器过程，并告诉集群管理器释放资源 集群管理器的主节点、从节点和Spark的驱动器、执行器节点是两个维度的概念； 集群的主从表示集群的中心化和分布式的部分Spark的执行器、驱动器节点描述的是执行Spark程序的两种进程的节点二者没有关联性，所以即使在YARN的工作节点上，Spark也是可以跑执行器和驱动器进程的 4.2 DAG、Jobs、Stage、Task详解 1）val input = sc.textFile(\"file:///tmp/input.txt\") 2）val tokenized = input.map(line => line.split(\" \")).filter(words => words.size>0) 3）val counts = tokenized.map(words => (words(0), 1)).reduceByKey((a,b) => a+b) 4）counts.collect() # 每一个RDD都记录了父节点的关系 scala> input.toDebugString res82: String = (2) file:///tmp/input.txt MapPartitionsRDD[62] at textFile at :27 [] | file:///tmp/input.txt HadoopRDD[61] at textFile at :27 [] scala> tokenized.toDebugString res83: String = (2) MapPartitionsRDD[64] at filter at :29 [] | MapPartitionsRDD[63] at map at :29 [] | file:///tmp/input.txt MapPartitionsRDD[62] at textFile at :27 [] | file:///tmp/input.txt HadoopRDD[61] at textFile at :27 [] scala> counts.toDebugString res84: String = (2) ShuffledRDD[66] at reduceByKey at :31 [] +-(2) MapPartitionsRDD[65] at map at :31 [] | MapPartitionsRDD[64] at filter at :29 [] | MapPartitionsRDD[63] at map at :29 [] | file:///tmp/input.txt MapPartitionsRDD[62] at textFile at :27 [] | file:///tmp/input.txt HadoopRDD[61] at textFile at :27 [] ## 因为reduceByKey是一个宽依赖，存在shuffle行为 stage_1 : HadoopRDD --> MapPartitionsRDD --> map --> filter --> map stage_2 : reduceByKey 一个job（counts.collect()）被拆成了两个stages 在行动操作之前，一切都是逻辑的DAG，行动操作是真实的物理变化发生时 驱动器程序执行了“流水线操作”，将多个RDD合并要一起执行 系谱图是自下而上的查找，这意味着如果任何一个父RDD上已经有数据缓存，这条链路都将得到优化 Spark的执行流程：用户代码定义DAG - 行动操作将DAG转转义为执行计划 - 任务在集群中调度并执行 4.3 执行器节点内存分配 默认60% RDD存储 cache() persist() 默认20% 数据清洗与聚合缓存数据混洗的输出数据，存储聚合的中间结果，通过spark.shuffle.memoryFraction来限定内存占比 默认20% 用户代码 与代码中的中间数据存储，比如创建数组 4.4 容错性 Spark会自动重新执行失败的 或 较慢的任务来应对有错误的或者比较慢的机器Spark还可能会在一台新的节点上投机的执行一个新的重复任务，如果提前结束，则提前获取结果，因此一个方法可能被执行多次 五、RDD编程 5.1 RDD是什么 RDD（弹性分布式数据集、Resilient Distributed Dataset）是Spark的数据结构。RDD的行为只分为三种：创建、转化（产生一个新的RDD）、行动（对当前RDD进行统计） 5.2 RDD创建 val local_lines = sc.textFile(\"file:///usr/local/opt/spark-latest-bin-hadoop2.4/README.md\") 或者 val local_lines = sc.parallelize(List(\"pandas\", \"i like pandas\") 5.3 RDD的转化 系谱图记录各个RDD之间的转换关系 5.3.1 针对各个元素的转化 # map() 针对每个元素一一对应的转换 scala> val numbers = sc.parallelize(List(1,2,3,4)); scala> numbers.map(x => x*x).collect().foreach(println) 1 4 9 16 # filter() 针对每个元素的过滤选择 scala> val numbers = sc.parallelize(List(1,2,3,4)); scala> numbers.filter(x => x>2).collect().foreach(println) 3 4 # flatmap() 对每一个元素处理后放回同一个大集合，典型的例子：split scala> val strings = sc.parallelize(List(\"huang#hai#feng\", \"zhong#guo\", \"huang#hai#feng\")); scala> strings.flatMap(x => x.split(\"#\")).collect().foreach(println) huang hai feng zhong guo huang hai feng # sample 采样， scala> numbers.collect().foreach(println) 1 2 3 4 # 每个位置按照随机种子，选 or 不选 scala> numbers.sample(false, 0.5).collect().foreach(println) 1 2 4 # 这个true，， 待理解 scala> numbers.sample(true, 0.5).collect().foreach(println) 2 3 3 4 5.3.2 伪集合操作 scala> numbers.collect().foreach(println) 1 2 3 4 scala> numbers_1.collect().foreach(println) 3 4 5 6 # union 并集，允许重复元素 scala> numbers.union(numbers_1).collect().foreach(println) 1 2 3 4 3 4 5 6 scala> numbers.union(numbers_1).distinct().collect().foreach(println) 4 1 5 6 2 3 # intersection交集 scala> numbers.intersection(numbers_1).collect().foreach(println) 4 3 # subtract差集 scala> numbers.subtract(numbers_1).collect().foreach(println) 2 1 # cartesian笛卡尔乘积 scala> numbers.cartesian(numbers_1).collect().foreach(println) (1,3) (1,4) (2,3) (2,4) (1,5) (1,6) (2,5) (2,6) (3,3) (3,4) (4,3) (4,4) (3,5) (3,6) (4,5) (4,6) 5.4 RDD的行动 # reduce scala> numbers.reduce((x, y) => x*y) res102: Int = 24 # countByValue word count已实现 scala> numbers.countByValue() res104: scala.collection.Map[Int,Long] = Map(4 -> 1, 2 -> 1, 1 -> 1, 3 -> 1) # fold 需要传入一个初始的单元值 加法是0 乘法是1 scala> numbers.fold(1)((x, y) => x*y) res103: Int = 24 # aggregate 求平均数 scala> val numbers = sc.parallelize(List(3,4,5,6)) numbers: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at :27 #numbers.aggregate((0,0))( # ((x, value) => (x._1 + value, x._2 + 1)), # ((x1, x2) => (x1._1+x2._1, x1._2+x2._2)) # ) scala> numbers.aggregate((0,0))(((x, value) => (x._1 + value, x._2 + 1)), ((x1, x2) => (x1._1+x2._1, x1._2+x2._2))) res0: (Int, Int) = (18,4) scala> res0._1/res0._2.toDouble res1: Double = 4.5 5.5 RDD的打印 take(n) 分区就近原则出scala> numbers.take(2).foreach(println) 1 2 top(n)scala> numbers.top(2).foreach(println) 按照数据集合自己的顺序出 4 3 sample(bWithReplacement, dFraction, seed) 丢骰子取样scala> numbers.sample(false, 0.3).foreach(println) 3 4 2 takeSample(bWithReplacement, n, seed) 随机取样n个scala> numbers.takeSample(false, 3).foreach(println) 4 3 1 collect() 全返回scala> numbers.collect().foreach(println) 1 2 3 4 scala> numbers.collect().mkString(\",\") res2: String = 1,2,3,4 5.6 持久化的几种类型 级别 使用的空间 CPU时间 是否在内存中 是否在磁盘上 备注 NONE DISK_ONLY 低 高 否 是 DISK_ONLY_2 低 高 否 是 同上一个级别，但存了两份 MEMORY_ONLY 高 低 是 否 MEMORY_ONLY_2 高 低 是 否 同上一个级别，但存了两份 MEMORY_ONLY_SER 低 高 是 否 ser是序列化的意思 MEMORY_ONLY_SER_2 低 高 是 否 同上一个级别，但存了两份 MEMORY_AND_DISK 高 中等 部分 部分 如果内存装不下了，多出了的写到磁盘 MEMORY_AND_DISK_2 高 中等 部分 部分 同上一个级别，但存了两份 MEMORY_AND_DISK_SER 低 高 部分 部分 内存存不下，多出来的部分存到磁盘，并将序列化数据写入内存 MEMORY_AND_DISK_SER_2 低 高 部分 部分 同上一个级别，但存了两份 OFF_HEAP 如果内存使用的不够了， 我们使用最少使用原则（LRU）进行回收 Spark还提供有unpersist()方法手动释放内存 六、Pair RDD编程 6.1 创建Pair RDD scala> numbers.collect().mkString(\",\") res7: String = 3,4,5,6 scala> val pairs = numbers.map(x => (x+1, x*x)) pairs: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[1] at map at :29 scala> pairs.collect().mkString(\",\") res9: String = (4,9),(5,16),(6,25),(7,36) val lines = sc.textFile(\"data.txt\") val pairs = lines.map(s => (s, 1)) val counts = pairs.reduceByKey((a, b) => a + b) 6.2 转化操作 scala> val pairs_1 = sc.parallelize(List((1, 2), (3, 4), (3, 6))) pairs_1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at :27 scala> pairs_1.collect.mkString(\",\") res12: String = (1,2),(3,4),(3,6) scala> pairs_1.reduceByKey((x, y) => x+y).collect.mkString(\",\") res13: String = (1,2),(3,10) scala> pairs_1.groupByKey().collect.mkString(\",\") res16: String = (1,CompactBuffer(2)),(3,CompactBuffer(4, 6)) scala> pairs_1.mapValues(x => x+10).collect.mkString(\",\") res17: String = (1,12),(3,14),(3,16) scala> pairs_1.flatMapValues(x => (x to 15)).collect.mkString(\",\") res19: String = (1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(1,8),(1,9),(1,10),(1,11),(1,12),(1,13),(1,14),(1,15),(3,4),(3,5),(3,6),(3,7),(3,8),(3,9),(3,10),(3,11),(3,12),(3,13),(3,14),(3,15),(3,6),(3,7),(3,8),(3,9),(3,10),(3,11),(3,12),(3,13),(3,14),(3,15) scala> pairs_1.keys.collect.mkString(\",\") res24: String = 1,3,3 scala> pairs_1.values.collect.mkString(\",\") res25: String = 2,4,6 scala> pairs_1.sortByKey().collect.mkString(\",\") res28: String = (1,2),(3,4),(3,6) scala> pairs_1.sortByKey(false).collect.mkString(\",\") res49: String = (3,4),(3,6),(1,2) ---------------------------- scala> pairs_1.sortByKey().collect.mkString(\",\") res28: String = (1,2),(3,4),(3,6) scala> val pairs_2 = sc.parallelize(List((3, 9))) pairs_2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[15] at parallelize at :27 scala> pairs_1.join(pairs_2).collect.mkString(\",\") res31: String = (3,(4,9)),(3,(6,9)) scala> pairs_1.rightOuterJoin(pairs_2).collect.mkString(\",\") res33: String = (3,(Some(4),9)),(3,(Some(6),9)) scala> pairs_1.leftOuterJoin(pairs_2).collect.mkString(\",\") res34: String = (1,(2,None)),(3,(4,Some(9))),(3,(6,Some(9))) scala> pairs_1.cogroup(pairs_2).collect.mkString(\",\") res35: String = (1,(CompactBuffer(2),CompactBuffer())),(3,(CompactBuffer(4, 6),CompactBuffer(9))) scala> pairs_1.filter{case(x, y) => y>4}.collect.mkString(\",\") res40: String = (3,6) 6.3 行动操作 scala> pairs_1.collect.mkString(\",\") res48: String = (1,2),(3,4),(3,6) ## 注意，返回的是一个Map scala> pairs_1.countByKey() res50: scala.collection.Map[Int,Long] = Map(1 -> 1, 3 -> 2) ## 注意，返回的是一个Map， 一个Key对应一个Value scala> pairs_1.collectAsMap() res52: scala.collection.Map[Int,Int] = Map(1 -> 2, 3 -> 6) ## 查询Value scala> pairs_1.lookup(3) res54: Seq[Int] = WrappedArray(4, 6) scala> pairs_1.lookup(3).toString res55: String = WrappedArray(4, 6) 6.4 分区详解 每一个RDD都是不可变的，每一个RDD我们都可以指定其分区方法 org.apache.spark.HashPartitioner(partitions : scala.Int) Hash分区 org.apache.spark.RangePartitioner[K, V] 范围分区 分区的好处不言而喻——减少数据的重新洗牌，大数据合并小数据集，小数据向着大数据集的分区靠拢，自然久省去了很多网络的耗时，一切就像是并行在单机上一样的分一次分区都会创建新的RDD分区完毕后还需要用到则需要使用缓存函数persist，避免每次都重新分区 scala> pairs_1.partitioner res56: Option[org.apache.spark.Partitioner] = None scala> pairs_1.partitionBy(new org.apache.spark.HashPartitioner(2)) res59: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[42] at partitionBy at :30 scala> pairs_1.partitioner res60: Option[org.apache.spark.Partitioner] = None scala> res59.partitioner res61: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@2) scala> pairs_1.sortByKey() res62: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[45] at sortByKey at :30 scala> res62.partitioner res63: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.RangePartitioner@8ed) 这里列出了所有会为生成的结果 RDD 设好分区方式的操作： cogroup() groupWith() join() leftOuterJoin() rightOuterJoin() groupByKey() reduceByKey() combineByKey() partitionBy() sort() mapValues()（如果父 RDD 有分区方式的话） flatMapValues()（如果父 RDD 有分区方式的话） filter()（如果父 RDD 有分区方式的话） 对于二元操作，输出数据的分区方式取决于父 RDD 的分区方式。默认情况下，结果会采用哈希分区，分区的数量和操作的并行度一样。如果其中的一个父 RDD 已经设置过分区方式，那么结果就会采用那种分区方式；如果两个父 RDD 都设置过分区方式，结果 RDD 会采用第一个父 RDD 的分区方式。但是分区数会选max action 是否会修改分区数 是否会修改分区方法 partitionBy(new HashPartitioner(n)) n HashPartitioner distinct 不变 none distinct(n) n none mapValues 不变 不变 reduceByKey 不变 不变 map 不变 none zipWithUniqueId 不变 none 七、文件操作 7.1 Spark支持的文件格式 格式名称 结构化 备注 文本文件 否 普通的文本文件，每行一条记录 JSON 半结构化 常见的基于文本的格式，大多数库都要求每行一条记录 CSV 是 非常常见的基于文本的格式，通常在电子表格应用中使用 SequenceFiles 是 一种用于键值对数据的常见 Hadoop 文件格式 Protocol buffers 是 一种快速、节约空间的跨语言格式 对象文件 是 用来将 Spark 作业中的数据存储下来以让共享的代码读取。改变类的时候 它会失效，因为它依赖于 Java 序列化 # 当传入的参数是目录的时候 ## 转化为一个RDD val input = sc.textFiles(inputFile) ## 转化为一个以文件名为Key的Pair RDD val input = sc.wholeTextFiles(inputFile) # 输出的产出参数是一个目录，因为Spark是并发输出的 rdd.saveAsTextFile(output_path) def main(args: Array[String]) { if (args.length if (JSON.parseObject(msg).getString(\"name\").contentEquals(\"Sparky The Bear\")) { msg } else { \"\" }).collect().foreach(print) input.map(JSON.parseObject(_)).saveAsTextFile(outputFile) } case class Person(name: String, favouriteAnimal: String) def main(args: Array[String]) { if (args.length val reader = new CSVReader(new StringReader(line)); reader.readNext(); } val people = result.map(x => Person(x(0), x(1))) val pandaLovers = people.filter(person => person.favouriteAnimal == \"panda\") pandaLovers.map(person => List(person.name, person.favouriteAnimal).toArray).mapPartitions{ people => val stringWriter = new StringWriter(); val csvWriter = new CSVWriter(stringWriter); csvWriter.writeAll(people.toList) Iterator(stringWriter.toString) }.saveAsTextFile(outputFile) } 7.2 Spark支持的文件存储方式 File System HDFS Cassandra HBase Amazon S3 Spark SQL etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. val lines = sc.textFile(\"file:///usr/local/opt/spark-latest-bin-hadoop2.4/README.md\") val lines = sc.textFile(\"hdfs:///usr/local/opt/spark-latest-bin-hadoop2.4/README.md\") val lines = sc.textFile(\"s3n://bigdata-east/tmp/README.md\") 八、Spark编程进阶 8.1 共享变量 8.1.1 累加器 accumulator 生命周期 在驱动器中创建 -- 在执行器中累计 -- 在驱动器中获取返回结果 累加器不是严格的只累计一次 转化操作可以因为一些原因被多次执行（任务执行失败重新执行、任务执行的太慢呗重新执行、原来RDD占用的内存被回收转化操作重新加载并执行方法），从而导致目前的累加器只适合做debug使用，或者foreach 累加器的操作需要满足交换律(即，a op b等同于b op a) 和 结合律(即、 (a op b) op c 等同于 a op (b op c)），比如加法、乘法、max函数 def main(args: Array[String]) { val master = args(0) val inputFile = args(1) val sc = new SparkContext(master, \"BasicLoadNums\", System.getenv(\"SPARK_HOME\")) val file = sc.textFile(inputFile) val errorLines = sc.accumulator(0) // Create an Accumulator[Int] initialized to 0 val dataLines = sc.accumulator(0) // Create a second Accumulator[Int] initialized to 0 val counts = file.flatMap(line => { try { val input = line.split(\" \") val data = Some((input(0), input(1).toInt)) dataLines += 1 data } catch { case e: java.lang.NumberFormatException => { errorLines += 1 None } case e: java.lang.ArrayIndexOutOfBoundsException => { errorLines += 1 None } } }).reduceByKey(_ + _) println(counts.collectAsMap().mkString(\", \")) println(s\"Too many errors ${errorLines.value} for ${dataLines.value}\") } 8.1.2 广播变量 调用SparkContext.broadcast创建出一个Broadcast[T]对象。 任何可序列化的类型都可以 通过value属性访问该广播变量的值 广播变量只会被发到各个节点一次，应作为只读值处理(但是，如果修改了这个值，将不会影响到别的节点) 传输中选择一个既好又快的序列化格式是很重要的 val signPrefixes = sc.broadcast(loadCallSignTable()) val countryContactCounts = contactCounts.map{ case (sign, count) => val country = lookupInArray(sign, signPrefixes.value) (country, count) }.reduceByKey((x, y) => x + y) def loadCallSignTable() = { scala.io.Source.fromFile(\"./files/callsign_tbl_sorted\").getLines() .filter(_ != \"\").toArray } # ./files/callsign_tbl_sorted 3DM, Swaziland (Kingdom of) 3DZ, Fiji (Republic of) 3FZ, Panama (Republic of) 3GZ, Chile 3UZ, China (People's Republic of) 8.2 调用第三方脚本 Pipe val pwd = System.getProperty(\"user.dir\") val distScript = pwd + \"/bin/finddistance.R\" val distScriptName = \"finddistance.R\" ## 上传脚本 sc.addFile(distScript) val pipeInputs = contactsContactLists.values.flatMap(x => x.map(y => s\"${y.contactlat},${y.contactlong},${y.mylat},${y.mylong}\")) println(pipeInputs.collect().toList) ## 根据脚本名加载文件 val distances = pipeInputs.pipe(SparkFiles.get(distScriptName)) 8.3 数值RDD - StatCounter 调用stats()时，会通过一次遍历数据计算出大多数常用的数据统计count() RDD 中的元素个数mean() 元素的平均值sum() 总和max() 最大值min() 最小值variance() 元素的方差sampleVariance() 从采样中计算出的方差stdev() 标准差sampleStdev() 采样的标准差 val stats = distanceDoubles.stats() val stddev = stats.stdev val mean = stats.mean @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 16:36:53 "},"spark/spark_running.html":{"url":"spark/spark_running.html","title":"Spark Running","keywords":"","body":"Spark_Running 一、工程打包 1.1 Maven mvn clean mvn package jar tf target/*jar 1.2 sbt sbt clean sbt package jar tf target/scala-2.10/*jar 二、Spark Submit 2.1 启动 $ ./bin/spark-submit --master yarn \\ --deploy-mode cluster \\ --driver-memory 4g \\ --executor-memory 2g \\ --executor-cores 1 \\ --queue thequeue \\ --jars my-other-jar.jar,my-other-other-jar.jar \\ --class org.apache.spark.examples.SparkPi \\ lib/spark-examples*.jar \\ app_arg1 app_arg2 #!\\bin\\bash SPARK_SUBMIT_SCRIPT=${SPARK_HOME}/bin/spark-submit ASSEMBLY_JAR=./target/*0.0.1.jar ${SPARK_SUBMIT_SCRIPT} \\ --class com.oreilly.learningsparkexamples.scala.WordCount \\ ${ASSEMBLY_JAR} local 2.2 Spark Conf参数配置 http://spark.apache.org/docs/latest/configuration.html#application-properties | 常用参数 | 描述 | | ----------------- | ------------------------------------------------------------ | | --master | 表示要连接的集群管理器 | | --deploy-mode | 选择驱动器程序的位置，1)本地客户端“client”，即在 spark-submit 被调用的这台机器上启动； 2）集群“cluster”，即驱动器程序会被传输并执行 于集群的一个工作节点上。默认是本地模式 | | --class | 运行 Java 或 Scala 程序时应用的主类 | | --name | 应用的显示名，会显示在 Spark 的网页用户界面中 | | --jars | 需要上传并放到应用的 CLASSPATH 中的 JAR | | --files | 需要放到应用工作目录中的文件的列表。这个参数一般用来放需要分发到各节点的 数据文件 | | --executor-memory | 执行器进程使用的内存量，以字节为单位。可以使用后缀指定更大的单位，比如 “512m”(512 MB)或“15g”(15 GB) | | --driver-memory | 驱动器进程使用的内存量，以字节为单位。可以使用后缀指定更大的单位，比如 “512m”(512 MB)或“15g”(15 GB) | Spark中可以设置参数的地方有四，优先级从高到低分别是 1、程序设定set // 创建一个conf对象 val conf = new SparkConf() conf.set(\"spark.app.name\", \"My Spark App\") conf.set(\"spark.master\", \"local[4]\") conf.set(\"spark.ui.port\", \"36000\") // 重载默认端口配置 // 使用这个配置对象创建一个SparkContext val sc = new SparkContext(conf) 2、spark submit 传参数指定 spark-submit \\ --class com.example.MyApp \\ --master local[4] \\ --name \"My Spark App\" \\ --conf spark.ui.port=36000 \\ myApp.jar 3、spark submit 传配置文件设定 spark-submit \\ --class com.example.MyApp \\ --properties-file my-config.conf \\ myApp.jar ## Contents of my-config.conf ## spark.master local[4] spark.app.name \"My Spark App\" spark.ui.port 36000 4、调用系统默认值 Spark 安装目录中找到 conf/spark-defaults.conf 文件，尝试读取该文件中以空格隔开的键值对数据 三、运行参数详解 3.1 --master http://spark.apache.org/docs/latest/submitting-applications.html | 值 | 描述 | | ----------------- | ------------------------------------------------------------ | | spark://host:port | 连接到指定端口的 Spark 独立集群上。默认情况下 Spark 独立主节点使用 7077 端口 | | mesos://host:port | 连接到指定端口的 Mesos 集群上。默认情况下 Mesos 主节点监听 5050 端口 | | yarn | 连接到一个 YARN 集群。当在 YARN 上运行时，需要设置环境变量 HADOOP _CONF_DIR 指向 Hadoop 配置目录，以获取集群信息 | | local | 运行本地模式，使用单核 | | local[N] | 运行本地模式，使用 N 个核心 | | local[*] | 运行本地模式，使用尽可能多的核心 | 四、集群管理器 4.1 Saprk自带的独立集群管理器 http://spark.apache.org/docs/latest/spark-standalone.html 启动独立集群管理器 (1) 将编译好的 Spark 复制到所有机器的一个相同的目录下，比如 /home/yourname/spark。 (2) 设置好从主节点机器到其他机器的 SSH 无密码登陆。这需要在所有机器上有相同的用 户账号，并在主节点上通过 ssh-keygen 生成 SSH 私钥，然后将这个私钥放到所有工作 节点的 .ssh/authorized_keys 文件中。如果你之前没有设置过这种配置，你可以使用如下 命令: # 在主节点上:运行ssh-keygen并接受默认选项 $ ssh-keygen -t dsa Enter file in which to save the key (/home/you/.ssh/id_dsa): [回车] Enter passphrase (empty for no passphrase): [空] Enter same passphrase again: [空] # 在工作节点上: # 把主节点的~/.ssh/id_dsa.pub文件复制到工作节点上，然后使用: $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys $ chmod 644 ~/.ssh/authorized_keys (3) 编辑主节点的 conf/slaves 文件并填上所有工作节点的主机名。 (4) 在主节点上运行 sbin/start-all.sh(要在主节点上运行而不是在工作节点上)以启动集群。 如果全部启动成功，你不会得到需要密码的提示符，而且可以在 http://masternode:8080 看到集群管理器的网页用户界面，上面显示着所有的工作节点。 (5) 要停止集群，在主节点上运行 bin/stop-all.sh。 检查管理器是否正常 spark-shell --master spark://masternode:7077 提交程序 spark-submit --master spark://masternode:7077 yourapp 管理界面 http://masternode:8080 正常情况下 (1)应用连接上了(即出现在了 Running Applications 中); (2) 列出的所使用的核心和内存均大于 0。 说明 执行器进程申请的内存(--executor-memory值)超过了集群所能提供的内存总量，独立集群管理器始终无法为应用分配执行器节点 独立集群管理器支持两种部署模式（--deploy-mode），client（默认，驱动器程序就是你提交任务的机器）和 cluster（驱动器程序运行在集群的某个工作节点） 如果你有一个集群（20台物理节点，每个节点4cores），当你提交一个任务（8cores，每个core1G），默认情况下，Spark将会在8台物理节点上召唤起8个core，每个core1G，当然我们也可以通过配置spark.deploy.spreadOut=false来要求申请尽可能少的物理节点，比如2台物理节点、2*4cores 4.2 YARN http://spark.apache.org/docs/latest/running-on-yarn.html 配置并提交任务(1) 设置环境变量 HADOOP_CONF_DIR。这个目录 包含 yarn-site.xml 和其他配置文件;如果你把 Hadoop 装到 HADOOP_HOME 中，那么这 个目录通常位于 HADOOP_HOME/conf 中，否则可能位于系统目录 /etc/hadoop/conf 中 (2) 然后用如下方式提交你的应用: export HADOOP_CONF_DIR=\"...\" spark-submit --master yarn yourapp 4.3 Mesos http://spark.apache.org/docs/latest/running-on-mesos.html 执行器之间的资源共享，分为细粒度模式（默认，执行器cores的数量会随着任务的执行而变化），和粗粒度模式（spark.mesos.coarse=true, 比较适合streaming这样的高实效性任务，减少core调度之间的延迟） 4.4 EC2 http://spark.apache.org/docs/latest/ec2-scripts.html 比较适合搭配S3 五、任务管理界面 Jobs \\ Stages 方便查看各个任务的执行时间 Storage 表示已缓存的RDD信息 Environment 可以查看我们设置的配置信息 Executors 各个节点的执行和配置情况 六、程序运行调优 6.1 优化分区数、并行度 在数据混洗的时候传合理的参指定并行度 对已有的数据进行从新分区 repartition、减少分区数coalesce # 以可以匹配数千个文件的通配字符串作为输入 >>> input = sc.textFile(\"s3n://log-files/2014/*.log\") >>> input.getNumPartitions() 35154 # 排除掉大部分数据的筛选方法 >>> lines = input.filter(lambda line: line.startswith(\"2014-10-17\")) >>> lines.getNumPartitions() 35154 # 在缓存lines之前先对其进行合并操作 >>> lines = lines.coalesce(5).cache() >>> lines.getNumPartitions() 4 # 可以在合并之后的RDD上进行后续分析 >>> lines.count() 6.2 设置kyro的系列化方式 org.apache.spark.serializer.KryoSerializer会优于默认的java序列化的库 普通的序列化 val conf = new SparkConf() conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") 有注册过的序列化 ``` val conf = new SparkConf() conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") conf.registerKryoClasses(Array(classOf[MyClass], classOf[MyOtherClass])) - 有强制要求必须注册的序列化 val conf = new SparkConf() conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") // 严格要求注册类 conf.set(\"spark.kryo.registrationRequired\", \"true\") conf.registerKryoClasses(Array(classOf[MyClass], classOf[MyOtherClass])) ``` 6.3 修改内存使用策略 1、重新分配RDD存储、数据混洗聚合存储、用户存储占比2、改进缓存策略，比方说MEMORY_ONLY 改为 MEMORY_AND_DISK，当数据缓存空间不够的时候就不会删除旧数据导致重新加载计算，而是直接从磁盘load数据；再比方说MEMORY_ONLY 改为 MEMORY_AND_DISK_SER 或者 MEMORY_ONLY_SER，虽然增加了序列化的时间，但是可以大量的减少GC的时间 6.4 硬件优化 1、双倍的硬件资源（CPU、Core）往往能带来应用时间减半的效果2、更大的本地磁盘可以帮助提高Spark的应用性能 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-11-27 13:33:24 "},"spark/spark_streaming_quick_start.html":{"url":"spark/spark_streaming_quick_start.html","title":"Spark Streaming Quick Start","keywords":"","body":"Spark Streaming Quick Start Spark Streaming的基本数据结构是DRDD（discretized stream），DRDD和RDD类似，但是引入了时间的概念。DRDD中三个最重要的时间变量 windowDuration，每次处理的时间窗口大小 slideDuration，本次处理和下一次处理之间的滑动时间间隔 batchInterval，DRDD里面每一个RDD缓存的数据时间 为了保证数据的可容灾性，DRDD里面新加了checkpoint的机制，数据可以从任何一个checkpoint恢复，以确保任务能够7*24小时执行 Spark Streaming的操作分为转化和输出，不同于RDD的转化和行动 官网 一、Demo /** * @description * 监听localhost的7777端口，处理每一行的输入 * nc -lk 7777 开启端口 发送数据 * 运行代码 * spark-submit --class com.hhf.spark.streaming.StreamingSocket ./sparkStreamingExample.jar 192.168.9.223 7777 * @version V1.0 * @author HHF * @time 2016-5-24 */ import org.apache.spark.streaming.dstream.DStream import org.apache.spark.SparkConf import org.apache.spark.streaming.Seconds import org.apache.spark.streaming.StreamingContext object StreamingSocket { def main(args: Array[String]) { if (args.length \") System.exit(1) } val conf = new SparkConf() conf.setAppName(\"StreamingSocket\") conf.setMaster(\"local\") // Create a StreamingContext with a 1 second batch size val ssc = new StreamingContext(conf, Seconds(10)) // Create a DStream from all the input on port 7777 val lines = ssc.socketTextStream(args(0), args(1).toInt) val errorLines = processLines(lines) // Print out the lines with errors, which causes this DStream to be evaluated errorLines.print() // start our streaming context and wait for it to \"finish\" ssc.start() // Wait for 10 seconds then exit. To run forever call without a timeout ssc.awaitTermination() ssc.stop() } def processLines(lines: DStream[String]) = { // Filter our DStream for lines with \"error\" lines.filter(_.contains(\"error\")) } } ## 建议直接走官网的详细demo 二、架构设计 参考来自《Saprk快速大数据分析》，很详细清楚的说明了spark streaning的执行过程。针对每一种数据源，都将有一个与之对应的长期运行的接收器，所以收到的数据都会有一个备份，任务的执行都将发生在副本上，万一数据丢失，将直接从接收器上重新将数据copy过来。 Spark Streaming的运行模式可以根据带不带window分为以下两种 2.1 每个batchInterval执行一次，无状态的运行 2.2 指定windowDuration、slideDuration的运行，每次移动特定的步伐，运行一个window里面的所有数据 三、转化 3.1 无状态转化 每批次的任务数据和之前的批次数据没有任何关系，无状态转化的操作和普通RDD的转化十分类似，强调transform函数，transform()的常见应用场景就是重用RDD的处理函数 val outlierDStream = accessLogsDStream.transform { rdd => extractOutliers(rdd) } 3.2 有状态转化 每批次的任务数据和之前的批次数据有交集，有交集就可以有优化，尽量使得所有的数据只需要被处理一次 3.2.1 基于于滑动窗口 val ipDStream = accessLogsDStream.map(logEntry => (logEntry.getIpAddress(), 1)) val ipCountDStream = ipDStream.reduceByKeyAndWindow( {(x, y) => x + y}, // for new {(x, y) => x - y}, // for old Seconds(30), // windowDuration Seconds(10) // slideDuration ) 3.2.2 状态更新 updateStateByKey 用来一直追踪一个事件的状态 def updateRunningSum(values: Seq[Long], state: Option[Long]) = { Some(state.getOrElse(0L) + values.size) } val responseCodeDStream = accessLogsDStream.map(log => (log.getResponseCode(), 1L)) val responseCodeCountDStream = responseCodeDStream.updateStateByKey(updateRunningSum _) 四、输出 五、容灾 5.1 checkpoint ssc.checkpoint(\"hdfs://...\") 5.2 driver程序容灾 使用StreamingContext.getOrCreate()函数，可以重新从检查点目录中初始化出streamingContext然后继续处理 StreamingContext.getOrCreate() spark独立管理器还可以设置参数来监控程序，自动拉起任务 5.3 executor节点容灾 数据在多个工作节点上备份 5.4 接收器容灾 将数据备份，允许一台工作节点数据丢失 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-11-27 15:53:43 "},"spark/spark_important.html":{"url":"spark/spark_important.html","title":"Spark划重点","keywords":"","body":"Spark划重点 一、Application 【重点，stage划分策略】 1.1 任务提交 - spark conf 任务从本地机器执行spark-submit命令将任务提交到集群 --master yarn | local[*] 指定集群管理器 --deploy-mode cluster 指定driver节点是本机还是集群上的任意节点 val conf = new SparkConf() val sc = new SparkContext(conf) 设置参数的地方有四，优先级从高到低分别是:程序set设定 -> spark summit 提交参数指定 -> 传配置文件设置(--properties-file) -> spark安装目录中设置conf/spark-defaults.conf 1.2 任务提交 - DAG 任务首先到driver节点上，driver节点负责任务的划分和调度，executor节点负责任务的执行和必要数据的缓存。程序RDD之间的转化定义了一张DAG有向无环图，driver节点将DAG转换成物理执行过程，划分成job、stage分发到各个executor节点上执行。 job， 每一个行动操作就是一个job，比如take、first、foreach stage，每一次宽依赖的转化(有shuffle)都会产生一个新的stage task，stage在executor上并行执行，分成的小任务就是task，task对应于一个partition数据 【重点，stage如何划分】 1、当触发rdd的action时: 比如collect，count、take、first、reduce、foreach2、当触发rdd的shuffle操作时: 比如repartition、coalesce、ByKey operations (except for counting) like groupByKey and reduceByKey, sortByKey and join operations like cogroup and join. Application --> Driver Program --> DAG --> Jobs --> Stages --> Taskset --> TaskScheduler --> Executors --> Tasks 1.3 partition划分 ## step1：读文件 val textFile = sc.textFile(\"/hhf/LICENSE\") ## step2: 创建hadoopFile def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat],classOf[LongWritable], classOf[Text],minPartitions).map(pair => pair._2.toString) } ## step3: 获得getPartitions， override def getPartitions: Array[Partition] = { val jobConf = getJobConf() // add the credentials here as this can be called before SparkContext initialized SparkHadoopUtil.get.addCredentials(jobConf) val inputFormat = getInputFormat(jobConf) if (inputFormat.isInstanceOf[Configurable]) { inputFormat.asInstanceOf[Configurable].setConf(jobConf) } val inputSplits = inputFormat.getSplits(jobConf, minPartitions) val array = new Array[Partition](inputSplits.size) for (i splits = new ArrayList(numSplits); NetworkTopology clusterMap = new NetworkTopology(); for (FileStatus file: files) { Path path = file.getPath(); long length = file.getLen(); if (length != 0) { FileSystem fs = path.getFileSystem(job); BlockLocation[] blkLocations; if (file instanceof LocatedFileStatus) { blkLocations = ((LocatedFileStatus) file).getBlockLocations(); } else { blkLocations = fs.getFileBlockLocations(file, 0, length); } if (isSplitable(fs, path)) { long blockSize = file.getBlockSize(); long splitSize = computeSplitSize(goalSize, minSize, blockSize); long bytesRemaining = length; while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) { String[] splitHosts = getSplitHosts(blkLocations, length-bytesRemaining, splitSize, clusterMap); splits.add(makeSplit(path, length-bytesRemaining, splitSize, splitHosts)); bytesRemaining -= splitSize; } if (bytesRemaining != 0) { String[] splitHosts = getSplitHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap); splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts)); } } else { String[] splitHosts = getSplitHosts(blkLocations,0,length,clusterMap); splits.add(makeSplit(path, 0, length, splitHosts)); } } else { //Create empty hosts array for zero length files splits.add(makeSplit(path, 0, length, new String[0])); } } LOG.debug(\"Total # of splits: \" + splits.size()); return splits.toArray(new FileSplit[splits.size()]); } ## step5: protected long computeSplitSize(long goalSize, long minSize, long blockSize) { return Math.max(minSize, Math.min(goalSize, blockSize)); } ## 解析 1、根据目标分区数、文件的总size，计算出第一个目标分区大小 —— goalSize long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits); numSplits 默认传参2 2、根据系统设置参数，计算出第二个最小分区大小 —— minSize long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE, 1),minSplitSize) minSplitSize 默认是1 private long minSplitSize = 1; 3、根据文件的块大小，计算出第三个分区大小 -- blockSize long blockSize = file.getBlockSize(); 4、确定最后的每个分区大小 return Math.max(minSize, Math.min(goalSize, blockSize)); ## 所以 1、创建HadoopRDD的时候传入的参数只能调小每个分区的大小 如，HadoopRDD一个blocksize是128M，假设一个文件大小是1G+10M，但是设置4个分区，那么实际在计算分区个数的时候结果为9个分区，最后一个分区大小为10M 2、一个分区最大的文件大小是一个块的大小 1.4 partition和block的关系 一个partition 对应一个task block位于存储空间、partition位于计算空间 block的大小是固定的、partition大小是不固定的 二、Shuffle【重点，shuffle的详细过程、shuffle的调参】 2.1 Hadoop Shuffle的过程（SortShuffleManager） 【过程总说】shuffle是map到reduce中间的一个数据传输和分配的过程，即，如何将map的数据产出，传输到reduce的输入。常用的shuffle策略比如hash，(a,1)(b,2)(c,3)分别传到reduce各个节点上，hash（a） 然后对reduce的个数取余数，分配不同的reduce并行执行后面的过程。 【过程详解】 Map端: DataNode --> InputSplit --> 环形内存缓冲区 --> partition、sort和combine --> 逾出到本地磁盘 --> Merge 第一步，Collect阶段每个map有一个环形内存缓冲区，用于存储任务的输出。默认大小100MB（io.sort.mb属性），一旦达到阀值0.8(io.sort.spill.percent)，进入Spill阶段 第二步，Spill阶段一个后台线程把内容写到(spill)Linux本地磁盘中的指定目录（mapred.local.dir）下的新建的一个溢出写文件。写磁盘前，线程首先根据reducer把数据进行partition，在每一个分区内部sort，如果有combiner，则会在sort后运行，使得map的输出更加紧凑。如果设置了还需要就行压缩 第三步，Merge阶段 map结束后所有的小文件会merge成一个大文件，并生成一个记录不同分区起始结束位置的索引文件。默认最大的每次合并文件数不超过10个（io.sort.factor），之前版本会将每个分区的数据存一份文件，产生大量的小文件，磁盘寻道非常耗时。 Reduce端: TaskTracker --> JobTracker --> Http Copy --> Merge --> Reducer 第四步，Copy阶段每一个map运行结束后TaskTracker会得到消息，进而将消息汇报给JobTracker（MR2中，任务直接通知master），reduce定时从JobTracker获取该信息，然后就开始了数据的copy过程，copy并不需要等待全部的map运行结束。reduce端可能从n个map的结果中获取数据，于是reduce端默认有5（mapred.reduce.parallel.copies）个数据复制线程从map端复制数据，走的是HTTP接口。数据先写到reduce端的堆缓存中，同样缓存占用到达一定阈值后会将数据写到磁盘中，写入之前如果有combiner，则在写入磁盘之前合并数据，减少数据的写入。如果指定了压缩方法，在内存中将会解压缩。 第五步，Merge阶段在远程copy数据的同时，Reduce Task在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多 第六步，Sort阶段 在合并的同时，也会做排序操作。由于各个Map Task已经实现对数据做过局部排序，故此Reduce Task只需要做一次归并排序即可保证copy数据的整体有序性，合并因子默认也是10（is.sort.factor），比如40个map结果，第一次合并4个文件。第二次、第三次、第四次合并10个文件，最后的10个文件直接输入给reduce 2.2 Spark Shuffle的过程 - 1.2.0之前是HashShuffle，之后是SortShuffle 顾名思义，hash和sort最大的不同就是前者没有走排序的过程，减少的运行中大量的时间消耗。一共有四个版本的shuffle。HashShuffle优化之前和优化之后的、SortShuffle的普通模式和bypass模式。 HashShuffle优化之前的版本每个map会产出reducer个output文件，导致运行大任务的时候产生超级多（core_num task_num reducer_num）的小文件，效率很低。 优化后，每个executor节点的每个core会针对每个reducer建立一个FileSegment，所有这个节点上的map任务的输出都将合并到这一个文件上，一共会生成（core_num * reducer_num）个文件 新的SortShuffle的输出文件数量为2*reduce_num，一份文件，一份index。sortShuffle 的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制（当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制） val bypassMergeThreshold: Int = conf.getInt(\"spark.shuffle.sort.bypassMergeThreshold\", 200) numPartitions 未优化的HashShuffle shuffle write会生成M*R个小文件，shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。 优化后的HashShuffle 设置spark.shuffle.consolidateFiles参数为true，开启优化之路。consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。shuffle write会生成Core_num*R个小文件 SortShuffle的普通运行机制 过程同Hadoop的shuffle是一样的，不过map的输出格式不再只是了，需要根据shuffle的算子来确定，如reduceByKey，则选用Map数据结构；如是join，则选用Array数据结构。在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。同样map的最后还有一个Merge的过程，输出是一个文件和一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。 bypass运行机制 bypass运行机制的触发条件如下：1、shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值（默认为200）2、不是排序类的shuffle算子（比如reduceByKey）执行流程和之前的一致，但是多个task共用了同一个buffer，每一个buffer都是针对一个reduce来创建的。数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。最后，将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销 2.3 Shuffle为什么耗时，应该怎么优化，如何调参 为什么耗时 数据完全是远程拷贝 采用HTTP协议进行数据传输 The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it. 因为shuffle设计到磁盘I/O、数据序列化、网络I/O，每一次shuffle都将分成一系列的map和reduce操作 Shuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. shuffle户生很多小文件，如果GC不常启动、任务又一直执行的时候就会消耗大量的磁盘 调参（原则：给shuffle尽可能多的内存）map端调参： io.sort.mb 缓冲队列 默认100m 调大，最优的情况就是不发生溢出 io.sort.spill.percent 默认0.8 缓冲队列溢出比例 io.sort.factor 默认是10 merge的最大合并文件数，调成100是很常见的事情 min.num.spills.for.combine 默认是3 如果设置了combiner。能触发combine的最小溢出文件数 mapred.compress.map.output 默认false，map的output是否需要压缩 spark.shuffle.memoryFraction 执行器节点分配给shuffle的内存占比，默认0.2，如果持久化用的比较少的话可以适当的调大这个比例 spark.shuffle.sort.bypassMergeThreshold 启动bypass方式的阈值 默认是200，当使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，减少了排序的性能开销 spark.shuffle.consolidateFiles 默认值false，表示HashShuffleManager的时候要不要合并map的output文件 reduce端调参： mapred.reduce.parallel.conpies 默认5 copy数据的线程数 io.sort.factor 默认10 最大的合并文件数 mapred.job.shuffle.input.buffer.percent 默认0.7 在shuffle阶段分配给reduce的堆内存占比 mapred.job.shuffle.merge.percent 默认0.66 reduce缓存溢出的占比 reduce段默认拉取文件大小一次48M，调大，减少请求连接次数 spark.shuffle.io.maxRetries reduce拉取数据失败的重试次数 默认是3，可以改到60 spark.shuffle.io.retryWait reduce 拉取数据失败重试的等待时间默认5s，可以调到60 2.4 Shuffle中的问题 数据倾斜，某个key对应的数据量特别大导致数据倾斜解决方案1、过滤少数导致倾斜的key2、提高shuffle操作的并行度3、局部聚合和全局聚合 比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4) 参考：Hadoop学习笔记—10.Shuffle过程那点事儿Hadoop深入学习：MapReduce的Shuffle过程详解Spark Shuffle原理、Shuffle操作问题解决和参数调优MapReduce过程、Spark和Hadoop以Shuffle为中心的对比分析HashShuffleManager解析Distributed Systems Architecture MapReduce过程、Spark和Hadoop以Shuffle为中心的对比分析Spark性能优化：shuffle调优 三、RDD介绍 3.1 partitions(分区) 每个RDD都有多个分区，每一个分区对应一个任务Task，每一个RDD都是不可修改的，所以一旦RDD确定，其分区数也是确定的，repartition会生成一个新的RDD 初始化读取文件的时候，HadoopPartition根据blockSize、minSize、goolSize来判断合适的splitSize，划分出totalSize／splitSize的分区，此时一个block对应了一个partition。 执行算子的时候如果指定并行度的话reduceByKey(XXX _, 18)，设置的是shuffle的时候reduce的并行度，也是result rdd的分区数 RDD分区选取策略 如果依赖的RDD中存在RDD已经设置了RDD.partitioner，则从设置了分区的RDD中则挑选出分区数最大的RDD.partitioner 如果依赖的所有RDD都没有设置RDD.partitioner，但是设置了Spark.default.parallelism，那么根据spark.default.parallelism设置创建HashPartitioner，作为ShuffledRDD的分区依据 以上2点都不满足，则从依赖的RDD中，去除分区数最大的RDD的分区个数，创建HashPartitioner，作为ShuffledRDD的分区依据 3.2 partitioner(分区方法) 目前只有两种HashPartitioner（默认）、RangePartitioner 3.3 dependencies(依赖关系) 窄依赖：父 RDD 的 partition 至多被一个子 RDD partition 依赖（OneToOneDependency，RangeDependency） 1个子RDD的分区对应于1个父RDD的分区，比如map，filter，union等算子 1个子RDD的分区对应于N个父RDD的分区，比如co-partioned join 宽依赖：父 RDD 的 partition 被多个子 RDD partitions 依赖（ShuffleDependency） 1个父RDD对应非全部多个子RDD分区，比如groupByKey，reduceByKey，sortByKey 1个父RDD对应所有子RDD分区，比如未经协同划分的join 在容灾恢复的时候，窄依赖的时候只需要恢复父RDD的一分分区，但是宽依赖会需要恢复多个分区，会带来一些数据的让费 窄依赖可以触发流水线操作所以我们尽量写窄依赖的程序 3.4 compute(获取分区迭代列表) 一个RDD有多么复杂，其最终都会调用内部的compute函数来计算一个分区的数据，compute是父RDD分区数据到子RDD分区数据的变换逻辑。 参考Spark核心RDD：计算函数computeSpark RDD之Partition怎样理解spark中的partition和block的关系? 四、spark调优 4.1 程序调优 1、优化并行度【parallel】 每一个分区对应一个map任务，并行度指的是reduce的任务个数 在数据混洗的时候传合理的参指定并行度 2、函数优化 优化分区【partition】 减少shuffle任务 减少副本 对已有的数据进行重新分区repartition、减少分区数coalesce 执行操作的时候继承父RDD的分区，减少shuffle 使用map-side预聚合的shuffle操作 reduceByKey/aggregateByKey替代groupByKeyreduceByKey/aggregateByKey底层使用combinerByKey实现，会在map端进行局部聚合；groupByKey不会 mapPartitions替代map、foreachPartitions替代foreachmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，在需要链接外部资源比如redis的时候会显得很高效。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上 repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的 优先使用broadcast 广播变量不会有副本，可以在大文件变量的时候减少副本储存传输，广播变量可以把变量从task级别提高到executor级别的共享 3、设置kyro的系列化方式 官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求提前注册好所有需要进行序列化的自定义类型，这种方式比较麻烦。 普通的序列化val conf = new SparkConf() conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") 有注册过的序列化val conf = new SparkConf() conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") conf.registerKryoClasses(Array(classOf[MyClass], classOf[MyOtherClass])) 有强制要求必须注册的序列化val conf = new SparkConf() conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") // 严格要求注册类 conf.set(\"spark.kryo.registrationRequired\", \"true\") conf.registerKryoClasses(Array(classOf[MyClass], classOf[MyOtherClass])) 4、缓存&缓存策略调优【persist】 缓存父类RDD有利于子类计算 改进缓存策略，比方说MEMORY_ONLY 改为 MEMORY_AND_DISK，当数据缓存空间不够的时候就不会删除旧数据导致重新加载计算，而是直接从磁盘load数据；再比方说MEMORY_ONLY 改为 MEMORY_AND_DISK_SER 或者 MEMORY_ONLY_SER，虽然增加了序列化的时间，但是可以大量的减少GC的时间 MEMORY_ONLY --> MEMORY_ONLY_SER --> MEMORY_AND_DISK_SER 4.2 参数调优 1、尽可能的将执行器节点分配在同一台物理机器上 设置参数spark.deploy.spreadOut=false 尽量减少物理节点的分配，如果你有一个集群（20台物理节点，每个节点4cores），当你提交一个任务（8cores，每个core1G），默认情况下，Spark将会在8台物理节点上召唤起8个core，每个core1G，设置为false后尽可能少的物理节点——2台物理节点、2*4cores 2、重新分配RDD存储、数据混洗聚合存储、用户存储占比，调大shuffle的内存占比 默认60% RDD存储 spark.storage.memoryFraction 默认20% 数据清洗与聚合 spark.shuffle.memoryFraction 默认20% 用户代码 与代码中的中间数据存储，比如创建数组 4.3机器调优 1、双倍的硬件资源（CPU、Core）往往能带来应用时间减半的效果 2、更大的本地磁盘可以帮助提高Spark的应用性能 参考 Spark算子选择策略 Spark性能优化指南——基础篇 五、Hadoop，Spark内部通讯 从akka到netty 启动Master并开启清空超时Worker的定时任务 Worker启动的时候，在preStart方法中连接Master,并向Master注册自己的相关信息 Master收到worker的注册并返回自己的url给Worker，表示该Worker注册成功 Worker收到注册成功的消息后，定时给Master发生心跳消息 为什么用netty代替akka主要原因是解决用户的Spark Application中akka版本和Spark内置的akka版本冲突的问题。比如，用户开发的Spark Application中用到了Spray框架，Spray依赖的akka版本跟Spark的不一致就会导致冲突，这个影响比较重要 六、重点函数突破 repartition VS coalesce repartition(numPartitions:Int):RDD[T]和coalesce(numPartitions:Int，shuffle:Boolean=false):RDD[T]，repartition只是coalesce接口中shuffle为true的简易实现 before numPartitions > after numPartitions此时只需要合并多个分区的数据即可，设置shuffle=false更为高效 before numPartitions >> after numPartitions考虑到合并的数据量太多了，建议使用shuffle=true before numPartitions 此时一定会发生shuffle，直接使用repartition persist VS cache cache方法等价于StorageLevel.MEMORY_ONLY的persist方法SparkContext中维护了一张哈希表persistRdds，用于登记所有被持久化的RDD，执行persist操作是，会将RDD的编号作为键，把RDD记录到persistRdds表中，unpersist函数会调用SparkContext对象的unpersistRDD方法，除了将RDD从哈希表persistRdds中移除之外，该方法还会将该RDD中的分区对于的所有块从存储介质中删除 combineByKey groupByKey,reduceByKey等的底层都是调用combineByKey实现的 def combineByKey[C]( createCombiner: V => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null) scala> val initialScores = Array((\"Fred\", 88.0), (\"Fred\", 95.0), (\"Fred\", 91.0), (\"Wilma\", 93.0), (\"Wilma\", 95.0), (\"Wilma\", 98.0)) initialScores: Array[(String, Double)] = Array((Fred,88.0), (Fred,95.0), (Fred,91.0), (Wilma,93.0), (Wilma,95.0), (Wilma,98.0)) scala> val d1 = sc.parallelize(initialScores) d1: org.apache.spark.rdd.RDD[(String, Double)] = ParallelCollectionRDD[0] at parallelize at :23 scala> type mytype = (Int, Double) defined type alias mytype scala> d1.combineByKey( | score => (1, score), | (c1: mytype, newScore) => (c1._1 + 1, c1._2 + newScore), | (c1: mytype, c2: mytype) => (c1._1 + c2._1, c1._2 + c2._2) | ).map { case (name, (num, socre)) => (name, socre / num) }.collect res0: Array[(String, Double)] = Array((Wilma,95.33333333333333), (Fred,91.33333333333333)) aggregate def aggregate[U](zeroValue: U) ( seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U ) (implicit arg0: ClassTag[U]) : U ## aggregate的执行过程 scala> rdd1.aggregate(2)( | {(x : Int,y : Int) => x + y}, | {(a : Int,b : Int) => a * b} | ) res18: Int = 1428 ##zeroValue=2 ##1、part_0中 zeroValue+5+4+3+2+1 = 2+5+4+3+2+1 = 17 ##1、part_1中 zeroValue+10+9+8+7+6 = 2+10+9+8+7+6 = 42 ##2、最后：zeroValue*part_0*part_1 = 2 * 17 * 42 = 1428 因此，zeroValue即确定了U的类型，也会对结果产生至关重要的影响，使用时候要特别注意。 七、数据倾斜 找出数据倾斜的原因 提高shuffle的并行度 两阶段聚合（局部聚合+全局聚合），先加随机前缀，后去掉前缀执行一遍全局聚合 reduce join 转为 map join，将小表缓存变成广播变量 随机前缀+扩展RDD，左边有数据倾斜的RDD进行随机前缀，右边小数据RDD扩展N倍，正常join操作 参考文章用实例说明Spark stage划分原理 Spark性能优化指南——高级篇 @ 学必求其心得，业必贵其专精@ WHAT - HOW - WHY@ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 17:30:23 "},"hadoop/hadoop_deploy_linux.html":{"url":"hadoop/hadoop_deploy_linux.html","title":"在Ubuntu_Centos上部署Hadoop","keywords":"","body":"在Ubuntu_Centos上部署Hadoop 一、用虚拟机模拟三台服务器 创建三台虚拟机并分别设置其独立静态IP 1.1设置虚拟机网络连接方式 将虚拟机网卡和主机之间的方式设置为桥接模式 1.2设置静态IP 打开文件/etc/sysconfig/network-scrip/ifcfg-eth0 DEVICE=”eth0” BOOTPROTO=\"static\" IPADDR=”本虚拟机静态IP” GATEWAY=”默认网关” NETMASK=”子网掩码” ONBOOT=”yes” 1.3重启网络服务命令 service network restart 二、添加Hadoop用户 2.1创建hhadoop用户组 （ 在centos上需要将addgroup改为groupadd） 2.2创建hadoop用户 （在centos需要将adduser改为useradd，并设置密码为hadoop）画 2.3给hadoop用户添加权限，打开/etc/sudoers文件 （centos可以直接使用vi代替gedit） 在root ALL=(ALL:ALL) ALL后添加hadoop ALL=(ALL:ALL) ALL 三、修改主机名 集群中需要给每台机器取个不同的名字。Ubuntu中机器名由 /etc/hostname文件决定。 3.1打开/etc/hostname文件 3.2回车后就打开/etc/hostname文件了，将/etc/hostname文件中的ubuntu改为你想取的机器名，如master、slave1、slave2。 3.3重启系统（reboot）后生效。 （如果是centos，则需要修改/etc/sysconfig/network文件） 四、修改host文件 将集群中各主机名对应的IP添加到每台主机的host文件中 五、添加无密码SSH服务 5.1安装openssh-server （注：自动安装openssh-server时，可能会进行不下去，可以先进行如下操作：) 更新的快慢取决于网速，如果中途因为时间过长中断了更新（ctrl+z），当再次更新时，会更新不了，报错为：“Ubuntu无法锁定管理目录（/var/lib/dpkg/），是否有其他进程占用它？“需要如下操作 操作完成后继续执行第1步 如果是centos则使用yum代替apt-get 5.2生成RSA对称密码 ssh-keygen -t rsa 回车后会在~/.ssh/下生成两个文件： id_rsa是私钥 id_rsa.pub是公钥 5.3进入~/.ssh/目录下，将id_rsa.pub追加到插头授权文件中 5.4本机测试 ( 注：当ssh远程登录到其它机器后，现在你控制的是远程的机器，需要执行退出命令才能重新控制本地主机。) 【切记】 SSH中密钥文件的权限值必须都设为600 sudo chmod 600 /home/hadoop/.ssh/ 六、节点间使用SSH连接 6.1将各节点的id_rsa.pub公钥拷贝到*各节点的authorized_keys内，就可以实现各节点之间的无SSH密码通讯 6.2测试连接 关于SSH的通讯原理，这里不做详述，有兴趣的话可以看看博客 http://qindongliang.iteye.com/blog/1958518 介绍的很详细的样子 七、安装JDK 我们选择的是jdk1.6.0_30版本,安装文件名为jdk-6u30-linux-i586.bin. 7.1复制jdk到安装目录 我们指定的安装目录是：/usr/local/java，先创建一个java文件夹 再将bin文件拷贝进去 7.2安装jdk 切换到root用户下 （注：如果因忘记密码而认证失败，可以先修改root用户的密码，再执行） 运行jdk-6u30-linux-i586.bin （注：如果遇到权限问题，可以先更改jdk-6u30-linux-i586.bin权限） 更改权限后再执行上一步，当看到下图情况时，说明你安装成功了。 这时在/usr/local/java目录下就多了一个jdk1.6.0_30文件夹 7.3配置环境变量 （1）打开/etc/profile文件 （2）添加变量如下： # /etc/profile: system-wide .profile file for the Bourne shell (sh(1)) # and Bourne compatible shells (bash(1), ksh(1), ash(1), ...). #set java environmentexport JAVA_HOME=/usr/local/java/jdk1.6.0_30export JRE_HOME=/usr/local/java/jdk1.6.0_30/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH（注：为了以后集群工作的方便，这里建议每台机器的java环境最好一致。） 一般更改/etc/profile文件后，需要重启机器才能生效。这里介绍一种不用重启使其生效的方法 （3）查看java环境变量是否配置成功 八、下载解压Hadoop 官网下载http://www.apache.org/dyn/closer.cgi/hadoop/core/ hadoop集群中每个机器上面的配置基本相同，所以我们先在master上面进行配置部署，然后再复制到其他节点。所以这里的安装过程相当于在每台机器上面都要执行。 【注意】：master和slaves安装的hadoop路径要完全一样，用户和组也要完全一致 解压文件到/usr路径下，并重命名 tar zxvf hadoop-2.2.0_x64.tar.gz mv hadoop-2.2.0 hadoop （解压命令在Ubuntu是 tar -zxf xxx.tar.gz，centos是tar zxvf xxx.tar.gz） 九、Hadoop配置文件 9.1创建必要文件夹 在master本地文件系统创建以下文件夹： ~/dfs/name ~/dfs/data ~/tmp 注意文件所属用户及用户组。如果不在新建的用户组下面，可以使用chown命令来修改：（chmod和chown命令的用法http://www.aboutyun.com/thread-7675-1-1.html) 9.2这里要涉及到的配置文件有7个 ~/hadoop-2.2.0/etc/hadoop/hadoop-env.sh ~/hadoop-2.2.0/etc/hadoop/yarn-env.sh ~/hadoop-2.2.0/etc/hadoop/slaves ~/hadoop-2.2.0/etc/hadoop/core-site.xml ~/hadoop-2.2.0/etc/hadoop/hdfs-site.xml ~/hadoop-2.2.0/etc/hadoop/mapred-site.xml ~/hadoop-2.2.0/etc/hadoop/yarn-site.xml 以上文件有些默认不存在的，可以复制相应的.template文件获得 9.3配置文件1：hadoop-env.sh 修改JAVA_HOME值（export JAVA_HOME=/usr/jdk1.7） 9.4配置文件2：yarn-env.sh 修改JAVA_HOME值（export JAVA_HOME=/usr/jdk1.7） 9.5配置文件3：slaves（这个文件里面保存所有slave节点） 写入以下内容： slave1 slave2 9.6配置文件4：core-site.xml ​ ​ fs.defaultFS ​ hdfs://master:8020 ​ ​ ​ io.file.buffer.size ​ 131072 ​ ​ ​ hadoop.tmp.dir ​ file:/home/hadoop/tmp ​ Abase for other temporary directories. ​ ​ ​ hadoop.proxyuser.aboutyun.hosts ​ * ​ ​ ​ hadoop.proxyuser.aboutyun.groups ​ * ​ 9.7配置文件5：hdfs-site.xml ​ ​ dfs.namenode.secondary.http-address ​ master:9001 ​ ​ dfs.namenode.name.dir ​ file:/home/hadoop/dfs/name ​ ​ dfs.datanode.data.dir ​ file:/home/hadoop/dfs/data ​ ​ ​ dfs.replication ​ 3 ​ ​ ​ dfs.webhdfs.enabled ​ true ​ 9.8配置文件6：mapred-site.xml ​ ​ mapreduce.framework.name ​ yarn ​ ​ ​ mapreduce.jobhistory.address ​ master:10020 ​ ​ ​ mapreduce.jobhistory.webapp.address ​ master:19888 ​ 9.9配置文件7：yarn-site.xml ​ ​ yarn.nodemanager.aux-services ​ mapreduce_shuffle ​ ​ yarn.nodemanager.aux-services.mapreduce.shuffle.class ​ org.apache.hadoop.mapred.ShuffleHandler ​ ​ ​ yarn.resourcemanager.address ​ master:8032 ​ ​ ​ yarn.resourcemanager.scheduler.address ​ master:8030 ​ ​ ​ yarn.resourcemanager.resource-tracker.address ​ master:8031 ​ yarn.resourcemanager.admin.address ​ master:8033 ​ ​ ​ yarn.resourcemanager.webapp.address ​ master:8088 ​ 9.10将配置文件复制到其他节点 上面配置完毕，我们基本上完成了90%了剩下就是复制。我们也可以把整个hadoop复制过去（注意slaves文件不一样） 【注意】拷贝配置文件的时候修改文件所有者 sudo chown hadoop mapred-site.xml 【记得】最后检查一下 cat hadoop-env.sh cat yarn-env.sh cat slaves cat core-site.xml cat hdfs-site.xml cat mapred-site.xml cat yarn-site.xml 9.11在每台服务器内配置环境变量/etc/profile export PATH=$PATH:/usr/local/hadoop/bin/ export PATH=$PATH:/usr/local/hadoop/sbin/ 十、启动Hadoop 10.1格式化namenode hdfs namenode –format 或则 hadoop namenode format 10.2启动hdfs start-dfs.sh 此时在master上面运行的进程有： namenode secondarynamenode slave节点上面运行的进程有：datanode 10.3启动yarn start-yarn.sh 我们看到如下效果： master有如下进程： slave1有如下进程 此时hadoop集群已全部配置完成！！！ 十一、查看Hadoop的Web管理窗口 11.1 ResourceManager 介绍：运行在主节点master上 网址：http://master:8088/ 配置文件：yarn-site.xml yarn.resourcemanager.webapp.address master:8088 【注】如果没有配置hosts，master将无法解析 要么你直接输入IP替代master，要么你就配置一下hosts，都很简单 Hosts文件地址C:\\Windows\\System32\\drivers\\etc 修改完后，正常的时候你会在浏览器里看到如下图 11.2 HDFS集群状态 网址：http://master:50070/ 11.3 NodeManager 介绍：运行在从节点上 网址：http://slave1:8042/ 11.4 JobHistory Server 启动：mr-jobhistory-daemon.sh start historyserver 网址：http://master:19888/ 配置：mapreduce.jobhistory.webapp.address 十二、运行实例WordCount 12.1 找到examples例子 在 /hadoop/share/hadoop/mapreduce下找到hadoop-mapreduce-examples-2.2.0.jar 12.2 运行前的准备工作 我们需要需要做一下运行需要的工作，比如输入输出路径，上传什么文件等。 ①先在HDFS创建几个数据目录： hadoop fs -mkdir -p /data/wordcount hadoop fs -mkdir -p /output/ 新建文件inputWord vi /usr/inputWord 新建完毕，查看内容： cat /usr/inputWord 目录/data/wordcount用来存放Hadoop自带的WordCount例子的数据文件， 任务的结果输出到/output/wordcount目录中。 ②将本地文件上传到HDFS中： hadoop fs -put /usr/inputWord /data/wordcount/ 查看上传后的文件情况 hadoop fs -ls /data/wordcount 查看文件的内容 hadoop fs -text /data/wordcount/inputWord 12.3 运行WordCount例子 hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /data/wordcount /output/wordcount 可以看到控制台输出程序运行的信息： aboutyun@master:~$ hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /data/wordcount /output/wordcount 14/05/14 10:33:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.16.77.15:8032 14/05/14 10:33:34 INFO input.FileInputFormat: Total input paths to process : 1 14/05/14 10:33:34 INFO mapreduce.JobSubmitter: number of splits:1 14/05/14 10:33:34 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class 14/05/14 10:33:34 INFO Configuration.deprecation: mapreduce.combine.class is deprecated. Instead, use mapreduce.job.combine.class 14/05/14 10:33:34 INFO Configuration.deprecation: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name 14/05/14 10:33:34 INFO Configuration.deprecation: mapreduce.reduce.class is deprecated. Instead, use mapreduce.job.reduce.class 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class 14/05/14 10:33:34 INFO Configuration.deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir 14/05/14 10:33:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1400084979891_0004 14/05/14 10:33:36 INFO impl.YarnClientImpl: Submitted application application_1400084979891_0004 to ResourceManager at master/172.16.77.15:8032 14/05/14 10:33:36 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1400084979891_0004/ 14/05/14 10:33:36 INFO mapreduce.Job: Running job: job_1400084979891_0004 14/05/14 10:33:45 INFO mapreduce.Job: Job job_1400084979891_0004 running in uber mode : false 14/05/14 10:33:45 INFO mapreduce.Job: map 0% reduce 0% 14/05/14 10:34:10 INFO mapreduce.Job: map 100% reduce 0% 14/05/14 10:34:19 INFO mapreduce.Job: map 100% reduce 100% 14/05/14 10:34:19 INFO mapreduce.Job: Job job_1400084979891_0004 completed successfully 14/05/14 10:34:20 INFO mapreduce.Job: Counters: 43 File System Counters FILE: Number of bytes read=81 FILE: Number of bytes written=158693 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=175 HDFS: Number of bytes written=51 HDFS: Number of read operations=6 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Launched reduce tasks=1 Data-local map tasks=1 Total time spent by all maps in occupied slots (ms)=23099 Total time spent by all reduces in occupied slots (ms)=6768 Map-Reduce Framework Map input records=5 Map output records=10 Map output bytes=106 Map output materialized bytes=81 Input split bytes=108 Combine input records=10 Combine output records=6 Reduce input groups=6 Reduce shuffle bytes=81 Reduce input records=6 Reduce output records=6 Spilled Records=12 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=377 CPU time spent (ms)=11190 Physical memory (bytes) snapshot=284524544 Virtual memory (bytes) snapshot=2000748544 Total committed heap usage (bytes)=136450048 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=67 File Output Format Counters Bytes Written=51 12.4查看结果 hadoop fs -text /output/wordcount/part-r-00000 结果数据示例如下： aboutyun@master:~$ hadoop fs -text /output/wordcount/part-r-00000 aboutyun 2 first 1 hello 3 master 1 slave 2 what 1 登录到Web控制台，访问链接http://master:8088/可以看到任务记录情况。 【转载博客】 为虚拟机设置静态IP http://java--hhf.iteye.com/admin/blogs/2100128 超详细单机版搭建hadoop环境图文解析 http://weixiaolu.iteye.com/blog/1401931 超详细在Ubuntu下安装JDK图文解析 http://weixiaolu.iteye.com/blog/1401786 hadoop2.2完全分布式最新高可靠安装文档 http://www.aboutyun.com/thread-7684-1-1.html hadoop2.X使用手册1：通过web端口查看主节点、slave1节点及集群运行状态 http://www.aboutyun.com/thread-7712-1-1.html hadoop2.2使用手册2：如何运行自带wordcount http://www.aboutyun.com/thread-7713-1-1.html 【问题集锦】 问题一： master: Error: JAVA_HOME is not set and could not be found. slave1: Error: JAVA_HOME is not set and could not be found. slave2: Error: JAVA_HOME is not set and could not be found. [hadoop@master hadoop]$ start-dfs.sh 14/08/18 08:51:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [master] master: Error: JAVA_HOME is not set and could not be found. slave1: Error: JAVA_HOME is not set and could not be found. slave2: Error: JAVA_HOME is not set and could not be found. Starting secondary namenodes [master] master: Error: JAVA_HOME is not set and could not be found. 14/08/18 08:51:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable [hadoop@master hadoop]$ $JAVA_HOME bash: /usr/local/java/jdk1.6.0_43: is a directory [hadoop@master hadoop]$ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-master.out slave1: Error: JAVA_HOME is not set and could not be found. slave2: Error: JAVA_HOME is not set and could not be found. [hadoop@master hadoop]$ jps 2158 ResourceManager 2415 Jps [hadoop@master hadoop]$ 解决办法： 改为 问题二：无法启动NodeManager NodeManager启动失败。查看日志，记录错误如下： 2014-02-10 18:24:07,635 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78) at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:196) at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager (NodeManager.java:329) at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:351) Caused by: org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78) at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.start(ContainerManagerImpl.java:248) ​ at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68) ​ ... 3 more Caused by: org.apache.hadoop.yarn.YarnException: Failed to check for existence of remoteLogDir [/var/log/hadoop-yarn/apps] ​ at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.verifyAndCreateRemoteLogDir(LogAggregationService.java:179) ​ at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.start(LogAggregationService.java:132) at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68) ​ ... 5 more 2014-02-10 18:24:07,647 INFO org.apache.hadoop.ipc.Server: Stopping server on 52154 日志显示：无法启动NodeManager，无法启动ContainerManager（也就是没有分配资源容器管理进程），也无法检查远程日志目录（在HDFS上），原因锁定，无法与Master（具体来说是ResourceManager）通信，然后到master上查看防火墙是否关闭，将Master上的防火墙关闭，并且chkconfig iptables off进行永久关闭（重启后不会自动开启），再去Slave节点上启动NodeManager 问题三：No route to host 配置Hadoop集群：java.net.NoRouteToHostException: 2012-07-04 18:43:31,479 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to /192.168.18.218:9000 failed on local exception: java.net.NoRouteToHostException: 没有到主机的路由 在配置hadoop的时候，很容易遇到以上错误，遇到以上问题的时候，一般可以通过以下几种方法解决。 1、从namenode主机ping其它主机名（如：ping slave1），如果ping不通,原因可能是namenode节点的/etc/hosts配置错误。 2、从datanode主机ping namenode主机名，如果ping不通,原因可能是datenode节点的/etc/hosts配置的配置错误。 3、查看namenode主机的9000（具体根据core-site.xml中的fs.default.name节点配置）端口，是否打开. vi /etc/sysconfig/iptables 打开配置文件加入如下语句: -A INPUT -p tcp -m state --state NEW -m tcp --dport 8080 -j ACCEPT重启防火墙 /etc/init.d/iptables restart 查看端口状态 /etc/init.d/iptables status 4、关闭系统防火墙。这是最容易出现的问题。用此命令service iptables stop关闭后，一切正常集群正常使用 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 16:36:53 "},"hadoop/hadoop_deploy_mac.html":{"url":"hadoop/hadoop_deploy_mac.html","title":"在Mac OS上部署Hadoop","keywords":"","body":"在Mac OS上部署Hadoop 1. 安装Homebrew和Cask 打开Mac终端, 安装OS X 不可或缺的套件管理器homebrew和homebrew cask $ ruby -e\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"$ brew install caskroom/cask/brew-cask 2. 安装Java Hadoop是由Java编写, 所以需要预先安装Java 6或者更高的版本 $ brew update && brew upgrade brew-cask && brew cleanup && brew cask cleanup$ brew cask installjava #测试是否安装成功 $ java -version 3. 配置SSH 为了确保在远程管理Hadoop以及Hadoop节点用户共享时的安全性, Hadoop需要配置使用SSH协议 首先在系统偏好设置->共享->打开远程登录服务->右侧选择允许所有用户访问 生成密钥对,执行如下命令 $ ssh-keygen -t rsa 执行这个命令后, 会在当前用户目录中的.ssh文件夹中生成id_rsa文件, 执行如下命令: $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 使用下面命令测试是否能够不使用密码登录 $ ssh localhost# Last login: Thu Mar 517:30:072015 4. 安装Hadoop $ brew install hadoop Hadoop会被安装在/usr/local/Cellar/hadoop目录下 4.1. 配置Hadoop 配置hadoop-env.sh```shell vim /usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hadoop-env.sh export HADOOP_OPTS=\"$HADOOP_OPTS -Djava.net.preferIPv4Stack=true\" 修改为: export HADOOP_OPTS=\"$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=\" - 配置core-site.xml ```shell vim /usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/core-site.xml hadoop.tmp.dir /usr/local/Cellar/hadoop/hdfs/tmp A base for other temporary directories. fs.default.name hdfs://localhost:9000 配置mapred-site.xml ```shell vim /usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/mapred-site.xml mapred.job.tracker localhost:9010 - 配置hdfs-site.xml ```shell vim /usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hdfs-site.xml dfs.replication 1value> 配置yarn-site.xml ```shell vim /usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/yarn-site.xml yarn.resourcemanager.address 127.0.0.1:8032 yarn.resourcemanager.scheduler.address 127.0.0.1:8030 yarn.resourcemanager.resource-tracker.address 127.0.0.1:8031 在运行后台程序前, 必须格式化新安装的HDFS, 并通过创建存储目录和初始化元数据创新空的文件系统, 执行下面命令: ```shell $ hadoop namenode -format 生成类似下面的字符串: DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.15/03/05 20:04:27 INFO namenode.NameNode: /************************************************************ STARTUP_MSG: Starting NameNode STARTUP_MSG: host = Andrew-liudeMacBook-Pro.local/192.168.1.100 STARTUP_MSG: args = [-format]STARTUP_MSG: version = 2.6.0... #此书省略大部分 STARTUP_MSG: java = 1.6.0_65 /***********************************************s************* SHUTDOWN_MSG: Shutting down NameNode at Andrew-liudeMacBook-Pro.local/192.168.1.100 ************************************************************/ 4.2. 启动后台程序 在/usr/local/Cellar/hadoop/2.6.0/sbin目录下, 执行如下命令 ./start-dfs.sh #启动HDFS$ ./stop-dfs.sh #停止HDFS 成功启动服务后, 可以直接在浏览器中输入http://localhost:50070/访问Hadoop页面 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 16:36:53 "},"hive/hive_quick_start.html":{"url":"hive/hive_quick_start.html","title":"Hive Quick Start","keywords":"","body":"Hive Quick Start 一、安装&启动 brew install hive brew install mysql mysql.server start 或者 brew services start mysql (启动) brew services stop mysql (停止) mysql -u root -p CREATE DATABASE hive; use hive; create user 'hive'@'localhost' identified by 'hive'； flush privileges； grant all privileges on hive.* to hive@localhost; flush privileges; hive --service metastore & hive --service hiveserver2 & hive shell 问题 Missing Hive Execution Jar: /usr/local/Cellar/hive/1.2.1/lib/hive-exec-*.jar【解决】修改下HIVE_HOME为export HIVE_HOME=/usr/local/Cellar/hive/1.2.1/libexec，问题不再现 Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083【解决】hive metastore进程已经有启动过了，kill掉 Exception in thread \"main\" java.lang.RuntimeException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7DFAILED: IllegalArgumentException java.net.URISyntaxException: Relative path in absolute URI: file:./$HIVE_HOME/iotmp/982c62b1-c776-4e95-9c91-a0b30e3b057b/hive_2016-06-28_17-58-00_856_4520712065933347856-1【解决】 hive.exec.scratchdir /tmp/mydir Scratch space for Hive jobs 把所有的${system:java.io.tmpdir}/${system:user.name}改为绝对路径 [Warning] could not update stats.【解决】 hive.stats.autogather false Failed with exception Unable to alter table. For direct MetaStore DB connections, we don't support retries at the client level.FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask【解决】 hive.insert.into.multilevel.dirs true 二、HQL语言 建表等 CREATE TABLE test_hive (a int, b int, c int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'; select * from test_hive; desc test_hive; CREATE TABLE test_hive_copy AS SELECT * FROM test_hive; CREATE TABLE test_hive_copy LIKE test_hive; 导入数据 LOAD DATA LOCAL INPATH '/usr/local/Cellar/hive/1.2.1/libexec/examples/files/test_hive.txt' OVERWRITE INTO TABLE test_hive; --本地文件 LOAD DATA INPATH '/hhf/test_hdfs.txt' INTO TABLE test_hive; --hdfs文件 > 导入后hdfs内的数据就没了，导入本地的文件后会上传到“hive.metastore.warehouse.dir”配置中 导出数据 insert overwrite local directory '/usr/local/Cellar/hive/1.2.1/libexec/examples/test_export_data' select * from test_hive; 数据类型原SQL数据类型都支持 新加了几个Struct{a INT; b INT}Map 键值对 Array'a', 'b', 'c' 操作大致一样order by 修改为 sort by 普通查询：排序，列别名，嵌套子查询 from(select a,b,c as c2 from test_hive)tselect t.a,t.c2where a>2111; 连接查询：JOIN..ON.. select t1.a,t1.b,t2.a,t2.bfrom tefrom test_hive t1 join test_hive_copy t2 on t1.a=t2.awhere t1.a>2111; 聚合查询：count, avg select count(*), avg(a) from test_hive; 聚合查询：GROUP BY, HAVING SELECT avg(a),b,sum(c) FROM test_hive GROUP BY b,c HAVING sum(c)>30 三、Hive交互式模式 quit,exit: 退出交互式shell reset: 重置配置为默认值 set = : 修改特定变量的值(如果变量名拼写错误，不会报错) set : 输出用户覆盖的hive配置变量 set -v : 输出所有Hadoop和Hive的配置变量 add FILE[S] , add JAR[S] , add ARCHIVE[S] * : 添加 一个或多个 file, jar, archives到分布式缓存 list FILE[S], list JAR[S], list ARCHIVE[S] : 输出已经添加到分布式缓存的资源。 list FILE[S] , list JAR[S] ,list ARCHIVE[S] * : 检查给定的资源是否添加到分布式缓存 delete FILE[S] ,delete JAR[S] ,delete ARCHIVE[S] * : 从分布式缓存删除指定的资源 ! : 从Hive shell执行一个shell命令 dfs : 从Hive shell执行一个dfs命令 : 执行一个Hive 查询，然后输出结果到标准输出 source FILE : 在CLI里执行一个hive脚本文件 四、与HDFS的交互 类型 描述 示例 TINYINT 1个字节（8位）有符号整数 1 SMALLINT 2字节（16位）有符号整数 1 INT 4字节（32位）有符号整数 1 BIGINT 8字节（64位）有符号整数 1 FLOAT 4字节（32位）单精度浮点数 1.0 DOUBLE 8字节（64位）双精度浮点数 1.0 BOOLEAN true/false true STRING 字符串 \"xia\" ## 创建hive表 hive -e \"create EXTERNAL table article_gmp (article_id string, product_id string, click double, request double, gmp double, last_update_time string, total_request string, language string) partitioned by (date string, min string) row format delimited fields terminated by '\\t' LOCATION '/inveno-projects/offline/article-gmp/data/article-gmp/history/';\" create EXTERNAL table test_uid (uid string) row format delimited fields terminated by '\\t' LOCATION '/user/haifeng.huang/profile'; ## 为hive表创建分区 hive -e \"alter table article_gmp add partition (date='20160917', min='2350') location '/inveno-projects/offline/article-gmp/data/article-gmp/history/20160917/2350/'\" ## 展示分区数 show partitions article_gmp ## 简单查询 select * from article_gmp where date='20160917' and min='2350' and article_id=\"1010497577\" and last_update_time is not null; select distinct get_json_object(json_string,'$.uid') from impression_reformat where date=20161007 and get_json_object(json_string,'$.article_impression_extra.content_id')=1021004753 ## 批量导入分区 /usr/lib/hive/bin/hive -e \"alter table article_gmp add partition (date='20160921', min='0310') location '/inveno-projects/offline/article-gmp/data/article-gmp/history/20160921/0310/' partition (date='20160921', min='0300') location '/inveno-projects/offline/article-gmp/data/article-gmp/history/20160921/0300/' partition (date='20160921', min='0250') location '/inveno-projects/offline/article-gmp/data/article-gmp/history/20160921/0250/' partition (date='20160921', min='0240') location '/inveno-projects/offline/article-gmp/data/article-gmp/history/20160921/0240/' partition (date='20160921', min='0230') location '/inveno-projects/offline/article-gmp/data/article-gmp/history/20160921/0230/' partition (date='20160921', min='0220') location '/inveno-projects/offline/article-gmp/data/article-gmp/history/20160921/0220/'\" @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 16:36:53 "},"hive/hive_join.html":{"url":"hive/hive_join.html","title":"Join解析","keywords":"","body":"Hive Join解析 一、各种不同的JOIN JOIN 说明 JOIN ON 内连接 LEFT [OUTER] JOIN ON 左连接 RIGHT [OUTER] JOIN ON 右连接 FULL [OUTER] JOIN ON 全连接 CROSS JOIN 交叉连接 LEFT SEMI JOIN ON 左主表连接 CREATE TABLE IF NOT EXISTS 0_a ( id string, name string ); CREATE TABLE IF NOT EXISTS 0_b ( id string, age INT ); ----------------------------------- --- 插入数据 ----------------------------------- INSERT INTO 0_a VALUES ('1', 'zhangsan'); INSERT INTO 0_a VALUES ('2', 'lisi'); INSERT INTO 0_a VALUES ('3', 'wangwu'); INSERT INTO 0_b VALUES ('1', 23); INSERT INTO 0_b VALUES ('2', 24); INSERT INTO 0_b VALUES ('4', 25); SELECT * FROM 0_a; SELECT * FROM 0_b; JOIN ON内连接，ON后面只能做等值匹配SELECT * FROM 0_a JOIN 0_b ON 0_a.id = 0_b.id 运行结果 0_a.id 0_a.name 0_b.id 0_b.age 1 zhangsan 1 23 2 lisi 2 24 LEFT [OUTER] JOIN ON左外连接，关键字OUTER可以不写SELECT * FROM 0_a LEFT JOIN 0_b ON 0_a.id = 0_b.id 运行结果 0_a.id 0_a.name 0_b.id 0_b.age 1 zhangsan 1 23 2 lisi 2 24 3 wangwu NULL NULL RIGHT [OUTER] JOIN ON右外连接，关键字OUTER可以不写SELECT * FROM 0_a RIGHT JOIN 0_b ON 0_a.id = 0_b.id 运行结果 0_a.id 0_a.name 0_b.id 0_b.age 1 zhangsan 1 23 2 lisi 2 24 NULL NULL 4 25 FULL [OUTER] JOIN ON全关联，简单的可以理解为LEFT JOIN + RIGHT JOIN，销重后的结果全关联不会使用MAPJOIN优化SELECT * FROM 0_a FULL JOIN 0_b ON 0_a.id = 0_b.id 运行结果 0_a.id 0_a.name 0_b.id 0_b.age 1 zhangsan 1 23 2 lisi 2 24 3 wangwu NULL NULL NULL NULL 4 25 CROSS JOIN笛卡尔乘积 n(左表的记录) m(右表的记录) = NM条记录数SELECT * FROM 0_a CROSS JOIN 0_b 运行结果 0_a.id 0_a.name 0_b.id 0_b.age 1 zhangsan 1 23 1 zhangsan 2 24 1 zhangsan 4 25 2 lisi 1 23 2 lisi 2 24 2 lisi 4 25 3 wangwu 1 23 3 wangwu 2 24 3 wangwu 4 25 LEFT SEMI JOIN ON左主表连接，只能查出左表的字段SELECT * FROM 0_a LEFT SEMI JOIN 0_b ON 0_a.id = 0_b.id 运行结果 0_a.id 0_a.name 1 zhangsan 2 lisi --等价于 SELECT a.id, a.name FROM 0_a a WHERE a.id IN (SELECT id FROM 0_b); --也等价于 SELECT a.id, a.name FROM 0_a a JOIN 0_b b ON a.id = b.id; --也等价于： SELECT a.id, a.name FROM 0_a a WHERE EXISTS ( SELECT 1 FROM 0_b b WHERE a.id = b.id ); 二、JOIN解析成MR任务 JOIN分成REDUCE JOIN 和 MAP JOIN 1）REDUCE JOIN reduce join 又可以称为common join、shuffle join、joinjoin的执行过程可以分为map、shuffle、reduce三个阶段 SELECT a.id,a.dept,b.age FROM a JOIN b ON (a.id = b.id); map 阶段 读取源数据表数据 解析成结构，key是where条件字段，value是表名tag+select需要查询出的字段内容 根据key排序 shuffle 阶段 根据key进行hash，根据hash值分配到不同的reduce节点 reduce 阶段 根据key进行join操作 2）MAP JOIN map join的执行流程中已经不再具有reduce阶段，和shuffle阶段，直接从map端输出结果 在客户端本地执行Task A，读取小表b的数据，将其转换成hash的结构 在客户端生成一个本地文件HashTableFile 将文件加载到DistributeCache中 启动mapTask扫描大表数据，每一条记录根据内存中缓存的b表数据计算，处理并输出结果 MAP JOIN带来的优势 小表连接大表，缩短运行时间 需要在join中使用不等式条件的时候，可以将on改为where，where就是在map阶段执行的 3）SMB(Sort-Merge-Buket) Join 适用于大表和大表做join，详见参考文章《Hive Sort Merge Bucket Map Join》 大牛博文章参考 Hive中Join的类型和用法Hive中Join的原理和机制Hive的三种Join方式hive优化之——控制hive任务中的map数和reduce数Hive Sort Merge Bucket Map JoinHive JOIN使用详解Hive SQL的编译过程 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-11-28 10:06:33 "},"hive/hive_window_function.html":{"url":"hive/hive_window_function.html","title":"分析窗口函数","keywords":"","body":"分析窗口函数 一、数据准备 ----------------------------------- --- 创建表 ----------------------------------- CREATE TABLE IF NOT EXISTS 0_test ( cookieid string, createtime string, pv INT ) COMMENT '测试分析函数表' ----------------------------------- --- 插入数据 ----------------------------------- SHOW CREATE TABLE 0_test; INSERT INTO 0_test VALUES ('cookie1', '2015-04-10', 1) INSERT INTO 0_test VALUES ('cookie1', '2015-04-11', 5); INSERT INTO 0_test VALUES ('cookie1', '2015-04-12', 7); INSERT INTO 0_test VALUES ('cookie1', '2015-04-13', 3); INSERT INTO 0_test VALUES ('cookie1', '2015-04-14', 2); INSERT INTO 0_test VALUES ('cookie1', '2015-04-15', 4); INSERT INTO 0_test VALUES ('cookie1', '2015-04-16', 4); INSERT INTO 0_test VALUES ('cookie2', '2015-04-14', 1); INSERT INTO 0_test VALUES ('cookie2', '2015-04-15', 2); INSERT INTO 0_test VALUES ('cookie2', '2015-04-16', 3); select * from 0_test; 二、统计窗口函数 窗口，也就是分组的概念，统计窗口函数就是分组后的统计函数。分组的关键字是PARTITION BY，相当于GROUP BY 如没有ORDER BY，统计的就是全组的数据， 如有ORDER BY，就是根据排序字段的一个累积统计 常用的使用场景比如： 统计每个组内某个字段最大的记录 统计每个组内某个字段在全组中的权重占比 统计每个组内某个字段的累积值等等 ----------------------------------- -- SUM ([DISTINCT] expr) OVER ([query_partition_clause] [order_by_clause]) -- AVG ([DISTINCT] expr) OVER ([query_partition_clause] [order_by_clause]) -- MAX ([DISTINCT] expr) OVER ([query_partition_clause] [order_by_clause]) -- MIN ([DISTINCT] expr) OVER ([query_partition_clause] [order_by_clause]) -- RATIO_TO_REPORT (expr) OVER ([query_partition_clause] [order_by_clause]) ----------------------------------- SELECT cookieid, createtime, pv, -- 默认为从起点到当前行的累加 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv_sum_1, -- 真个分组内的累加 SUM(pv) OVER(PARTITION BY cookieid) AS pv_sum_2, AVG(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv_avg_1, AVG(pv) OVER(PARTITION BY cookieid) AS pv_avg_2, MAX(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv_max_1, MAX(pv) OVER(PARTITION BY cookieid) AS pv_max_2, MIN(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv_min_1, MIN(pv) OVER(PARTITION BY cookieid) AS pv_min_2, -- 计算数值占比 RATIO_TO_REPORT(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv_ratio_to_report_1, RATIO_TO_REPORT(pv) OVER(PARTITION BY cookieid) AS pv_ratio_to_report_2 FROM 0_test 运行结果 cookieid createtime pv pv_sum_1 pv_sum_2 pv_avg_1 pv_avg_2 pv_max_1 pv_max_2 pv_min_1 pv_min_2 pv_ratio_to_report_1 pv_ratio_to_report_2 cookie1 2015-04-10 1 1 26 1.0 3.7142857142857144 1 7 1 1 1.0 0.038461538461538464 cookie1 2015-04-11 5 6 26 3.0 3.7142857142857144 5 7 1 1 0.8333333333333334 0.19230769230769232 cookie1 2015-04-12 7 13 26 4.333333333333333 3.7142857142857144 7 7 1 1 0.5384615384615384 0.2692307692307692 cookie1 2015-04-13 3 16 26 4.0 3.7142857142857144 7 7 1 1 0.1875 0.11538461538461539 cookie1 2015-04-14 2 18 26 3.6 3.7142857142857144 7 7 1 1 0.1111111111111111 0.07692307692307693 cookie1 2015-04-15 4 22 26 3.6666666666666665 3.7142857142857144 7 7 1 1 0.18181818181818182 0.15384615384615385 cookie1 2015-04-16 4 26 26 3.7142857142857144 3.7142857142857144 7 7 1 1 0.15384615384615385 0.15384615384615385 cookie2 2015-04-14 1 1 6 1.0 2.0 1 3 1 1 1.0 0.16666666666666666 cookie2 2015-04-15 2 3 6 1.5 2.0 2 3 1 1 0.6666666666666666 0.3333333333333333 cookie2 2015-04-16 3 6 6 2.0 2.0 3 3 1 1 0.5 0.5 # 为了加深分组|窗口的理解，单独介绍一下count over函数，统计当前分组的内的次数 ----------------------------------- --- COUNT( 1 | [DISTINCT] expr) OVER ([query_partition_clause] [order_by_clause]) ----------------------------------- SELECT cookieid, COUNT(1) OVER(PARTITION BY cookieid) AS count_over_1, COUNT(2) OVER(PARTITION BY cookieid) AS count_over_2, COUNT(20) OVER(PARTITION BY cookieid) AS count_over_20, COUNT(DISTINCT pv) OVER(PARTITION BY cookieid) AS count_over_distinct_pv FROM 0_test 运行结果 | cookieid | count_over_1 | count_over_2 | count_over_20 | count_over_distinct_pv | | -------- | ------------ | ------------ | ------------- | ---------------------- | | cookie1 | 7 | 7 | 7 | 6 | | cookie1 | 7 | 7 | 7 | 6 | | cookie1 | 7 | 7 | 7 | 6 | | cookie1 | 7 | 7 | 7 | 6 | | cookie1 | 7 | 7 | 7 | 6 | | cookie1 | 7 | 7 | 7 | 6 | | cookie1 | 7 | 7 | 7 | 6 | | cookie2 | 3 | 3 | 3 | 3 | | cookie2 | 3 | 3 | 3 | 3 | | cookie2 | 3 | 3 | 3 | 3 | # 类似的常规SQL SELECT cookieid, COUNT(1) AS count_over_1, COUNT(2) AS count_over_2, COUNT(20) AS count_over_20, COUNT(DISTINCT pv) AS count_over_distinct_pv FROM 0_test GROUP BY cookieid 运行结果 | cookieid | count_over_1 | count_over_2 | count_over_20 | count_over_distinct_pv | | -------- | ------------ | ------------ | ------------- | ---------------------- | | cookie1 | 7 | 7 | 7 | 6 | | cookie2 | 3 | 3 | 3 | 3 | 区别 使用带有OVER的窗口函数查询出来的行数等于原始数据行数，而GROUP BY行数为分组的组数 窗口函数在不带ORDER BY语句的时候使用方法和效果同基本函数 三、排序窗口函数 ROW_NUMBER、DENSE_RANK、RANK都是进行组内排序，他们可以为每一行输出一个组内的顺序编号。区别在于 ROW_NUMBER 每一行都有一个唯一的编号 DENSE_RANK 每一行都有一个编号，数据相同的并列为一个编号，下一行数据编号连续，比如两个并列第三名，下一个就是第四名 RANK 每一行都有一个编号，数据相同的并列为一个编号，下一行数据编号不连续，比如两个并列第三名，下一个就是第五名常用的使用场景比如 统计每个班每个同学的成绩排名 统计每个组根据某个字段排序的第N条记录 ----------------------------------- -- ROW_NUMBER() OVER([query_partition_clause] order_by_clause) -- DENSE_RANK() OVER([query_partition_clause] order_by_clause) -- RANK() OVER([query_partition_clause] order_by_clause) ----------------------------------- SELECT cookieid, createtime, pv, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv desc) AS pv_row_number, DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS pv_dense_rank, RANK() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS pv_rank FROM 0_test 运行结果 cookieid createtime pv pv_row_number pv_dense_rank pv_rank cookie1 2015-04-12 7 1 1 1 cookie1 2015-04-11 5 2 2 2 cookie1 2015-04-16 4 3 3 3 cookie1 2015-04-15 4 4 3 3 cookie1 2015-04-13 3 5 4 5 cookie1 2015-04-14 2 6 5 6 cookie1 2015-04-10 1 7 6 7 cookie2 2015-04-16 3 1 1 1 cookie2 2015-04-15 2 2 2 2 cookie2 2015-04-14 1 3 3 3 四、位移窗口函数 在每个分组中，每行记录都有自己的顺序，也就有了位移的概念。 LAG OVER函数可以方便的获得自己前N行记录的某个字段的内容， LEAD OVER与之相反，取的是后N行的记录数据所有offset必然都是一个正整数，值得注意的是，每行记录在每个分组中都会有自己的顺序，统计值根据位移来统计，所以如果最后输出的顺序不一致的话可能导致每个组的统计位移和最后输出的顺序不一致的情况，建议加上ORDER BY 使用场景比如： 统计每一组中最早加入的人 ```sql -- LAG (value_expr [, offset] [, default]) OVER ([query_partition_clause] order_by_clause) -- LEAD (value_expr [, offset] [, default]) OVER ([query_partition_clause] order_by_clause) -- FIRST_VALUE (expr) OVER ([query_partition_clause] [order_by_clause]) -- LAST_VALUE (expr) OVER ([query_partition_clause] [order_by_clause]) SELECT cookieid, createtime, pv, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rk, -- 取本组中，前两行的数据，没取到的话就填默认值, 没有默认值的话填NULL LAG(createtime, 2, 'default') OVER(PARTITION BY cookieid ORDER BY createtime) AS lag_createtime, -- LEAD完全和LAG相关，往下N行取数据 LEAD(createtime, 2, 'default') OVER(PARTITION BY cookieid ORDER BY createtime) AS lead_createtime, FIRST_VALUE(createtime) OVER(PARTITION BY cookieid ORDER BY createtime) AS first_createtime, LAST_VALUE(createtime) OVER(PARTITION BY cookieid ORDER BY createtime) AS last_createtimeFROM 0_test ORDER BY cookieid, createtime ``` 运行结果 cookieid createtime pv rk lag_createtime lead_createtime first_createtime last_createtime cookie1 2015-04-10 1 1 default 2015-04-12 2015-04-10 2015-04-16 cookie1 2015-04-11 5 2 default 2015-04-13 2015-04-10 2015-04-16 cookie1 2015-04-12 7 3 2015-04-10 2015-04-14 2015-04-10 2015-04-16 cookie1 2015-04-13 3 4 2015-04-11 2015-04-15 2015-04-10 2015-04-16 cookie1 2015-04-14 2 5 2015-04-12 2015-04-16 2015-04-10 2015-04-16 cookie1 2015-04-15 4 6 2015-04-13 default 2015-04-10 2015-04-16 cookie1 2015-04-16 4 7 2015-04-14 default 2015-04-10 2015-04-16 cookie2 2015-04-14 1 1 default 2015-04-16 2015-04-14 2015-04-16 cookie2 2015-04-15 2 2 default default 2015-04-14 2015-04-16 cookie2 2015-04-16 3 3 2015-04-14 default 2015-04-14 2015-04-16 大牛文章参考 Hive分析窗口函数(一) SUM,AVG,MIN,MAXHive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANKHive分析窗口函数(三) CUME_DIST,PERCENT_RANKHive分析窗口函数(四) LAG,LEAD,FIRST_VALUE,LAST_VALUEHive分析窗口函数(五) GROUPING SETS,GROUPING__ID,CUBE,ROLLUPoracle 分析函数 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 16:36:53 "},"hive/hive_optimize.html":{"url":"hive/hive_optimize.html","title":"SQL优化","keywords":"","body":"SQL优化 执行过程优化 1、尽量不使用笛卡尔乘积 笛卡尔积数据量大，被分配的reduce任务个数少，耗时严重以下两种形式的SQL会导致全表笛卡尔积： select * from A, B where A.key = B.key and A.key > 10; select * from A join B where A.key = B.key and A.key > 10; 正确写法 select * from A join B on A.key = B.key where A.key > 10; 多个数据表连接时，也要注意将on内写 A join B join tablec join ... on A.col1 = B.col2 and ... 改为 A join B on A.key = B.key and A.key > 10 join C on A.key = C.key join ... on ... 2、尽量使用map join 小表join大表的时候，使用mapjoin把小表放到内存中处理，语法很简单只需要增加 /+ MAPJOIN(pt) / ，把需要分发的表放入到内存中 SELECT 0_test.pv, 1_test.pv FROM 0_test JOIN 1_test ON 0_test.cookieid = 1_test.cookieid 改为 SELECT /*+ mapjoin(0_test)*/ 0_test.pv, 1_test.pv FROM 0_test JOIN 1_test ON 0_test.cookieid = 1_test.cookieid 考虑到map join比较消耗内存，不支持多表连接在开始执行hash map join之前，需要对部分参数进行设置。主要包括： hive.mapjoin.cache.numrowshash map join会优先将计算所需的小表键值对保存在内存容器中，剩下的键值对将被保存到硬盘。该参数决定保存在内存中的记录条数，系统默认设置该参数为500000。该条数越多，所需的内存越多，则需要将mapred.child.java.opts设的越大。TDW中的默认hash分区数为500，因此，用户可以根据小表的行数算出将该参数设为多少较合适。示例如下： set hive.mapjoin.cache.numrows=3000000 mapred.child.java.opts该参数设置map任务和reduce任务可用的虚拟机最大内存。由于map join是一个比较消耗内存的操作，而默认的内存大小为1G，即1024M，因此，在执行上述hash map join查询命令之前最好先用set命令设置内存大小。根据经验数据，在列数不多的情况下，一百万条记录所需的内存容器大小约为500~700MB.因此，如果在内存中缓冲的数据条数为300万，则将该参数设为3072M较为稳妥。原则上，mapred.child.java.opts的大小不要超过4096M。如果单个map任务要处理的条数较多，超过内存限制，则可以根据可用内存大小设置hive.mapjoin.cache.numrows，超出部分将被放入硬盘文件缓存。示例如下： set mapred.child.java.opts=-Xmx4096M； Sorted.Merge.Map.JoinTDW中提供了两种hash map join算法，两者的区别是一种算法使用hash map作为内存容器，而后一种使用数组（sorted list）作为内存容器。前者是TDW中的默认hash map join算法，后者的速度更快，对内存的需求更少。但是，后者要求小表的每个hash分区中的数据必须是有序的，且如果小表的hash分区是二级分区的话不能有多个一级分区参与计算。如要使用基于sorted list的算法,set Sorted.Merge.Map.Join = true； 3、及时掌握表的数据情况 # 查看每个分区数据量大小 show rowcount extended A # 查看每个分区数据大小，单位是字节 show tablesize extended A table_partition size pri partitions NULL default 10 row count: 10 4、全局distinct、count SELECT COUNT(DISTINCT key) FROM A WHERE key >= 20121001 改为 SELECT COUNT(key) FROM ( SELECT DISTINCT key AS key FROM A WHERE key >= 20121001 ) 虽然修改后的方法繁琐一些，但是执行效率高很多，前者只会有一个reduce的任务，在数据量大的时候问题比较严重 5、全局的group by， count SELECT count(1) FROM A; 改写成 SELECT pt,count(1) FROM A GROUP BY pt; 5、减少使用exists 和 not exists exists内部是join实现的， NOT EXISTS改写为left join加is null SELECT a FROM A WHERE a.key not exists (# exists 后面的子查询返回的是一个boolean值，和select查询的出来的字段没有任何关系，只要where条件通过即可 select 1 from B where A.col1=B.col2 and B.col3=… and… ) 改写为 SELECT a FROM A LEFT JOIN B ON A.col1=B.col2 and B.col3=… and … Where … and b.col2 is null exists改写成left simi join（左主表连接，只能查出左表的字段） SELECT … FROM A WHERE … AND exists ( SELECT 1 FROM B WHERE A.col1=B.col2 AND B.col3=… AND… ) 改写 SELECT … FROM A left semi join B on A.col1=B.col2 AND B.col3=… AND … WHERE … 改写后，根据各表的数据量大小调整join顺序，保证小表连接大表，或者使用map join 6、把控MAP REDUCE数量 原则上需要因地制宜，根据具体任务实地调优，让每个map处理合适的数据量、配置合理的map、reduce个数。 map端使大数据量利用合适的map数；使单个map任务处理合适的数据量 先搞清楚map数怎么来的根据hadoop file split方法来切割map数，针对每个文件，切割成map数为（文件大小/文件块大小 + 1），如文件块为64M，输入为两个文件，70M和20M，将切割成3个map（64M + 6M + 20M） 调整map数map数太多，每个map处理的数据量小，则大量的时间浪费在map的启动和初始化上； 所以在执行任务之前先对map数归并一下 set mapred.max.split.size=100000000; set mapred.min.split.size.per.node=100000000; set mapred.min.split.size.per.rack=100000000; set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; map数太少，则资源浪费，单个map处理时间太长。所以强行的将map数加大 set mapred.reduce.tasks=10; reduce端使大数据量利用合适的reduce数；使单个reduce任务处理合适的数据量 先搞清楚reduce数怎么来的 hive.exec.reducers.bytes.per.reducer（每个reduce任务处理的数据量，默认为1G） hive.exec.reducers.max（每个任务最大的reduce数，默认为999） 所以，如果map的输出有5.5G，将只有6个reduce。有以下几种情况会导致只有一个reduce a) map端的输出b) 没有group by的汇总，比如把select pt,count(1) from A group by pt; 写成 select count(1) from A;c) 用了Order byd) 有笛卡尔积 调整reduce个数修改每个reduce处理的数据量 set hive.exec.reducers.bytes.per.reducer=500000000; （500M） 强制指定reduce个数 set mapred.reduce.tasks = 15; 7、避免不同的数据结构发生对比 string和bigint比较的时候，reduce会发生数据倾斜，请先cast强制转换格式 SELECT t2.inewlevel8 as snewlevel8,t1.skey,t1.svalue from ( SELECT suin from tA )t1 join ( SELECT cast(iuin as STRING) as suin FROM tB ) t2 on (t1.suin = t2.suin) 大牛博客参考 控制hive任务中的map数和reduce数hive计算map数和reduce数 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 16:36:53 "},"kafka/kafka_quick_start.html":{"url":"kafka/kafka_quick_start.html","title":"Kafka Quick Start","keywords":"","body":"Kafka Quick Start 一、官方文档 特性介绍 Introduction安装上手 Quickstart > tar -xzf kafka_2.11-0.10.0.0.tgz > cd kafka_2.11-0.10.0.0 > bin/zookeeper-server-start.sh config/zookeeper.properties > bin/kafka-server-start.sh config/server.properties > bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test > bin/kafka-topics.sh --list --zookeeper localhost:2181 > bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test > bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning > bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test 二、关键词定义&结构 名词 定义 producer 生产者 consumer 消费者 consumer group 消费者组 broker 集群中的单个实例 topic 一类信息，会分成多个partition(区) partition 在存储层面是append log文件 offset 一个long型数字，唯一标记一条消息 log生命周期 消费后不会立即删除消息，设定固定的消息删除时间 1、partition 每个partition有两个角色，leader和follower，leader负责所有的读写请求，follower负责容灾，当leader 出现问题时，优先从ISR列表中选出一个新的leader 2、consumer Kafka提供了两套consumer api，分为high-level api和sample-api。Sample-api 是一个底层的API，它维持了一个和单一broker的连接，并且这个API是完全无状态的，每次请求都需要指定offset值，同一个group也可以多次消费相同的数据。Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。 3、清理数据 一是基于时间，二是基于Partition文件大小 # The minimum age of a log file to be eligible for deletion log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining # segments don't drop below log.retention.bytes. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 4、delivery guarantee —— At least one At most once 消息可能会丢，但绝不会重复传输At least one 消息绝不会丢，但可能会重复传输Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的 三、kafka数据写入和检索 1、数据存储结构 topic -> partition -> segment -> .index | .log -> offset partition是一个目录名称，命名规则 —— topic加序号 每一条消息都有一个key —— 也就是offset，key可以作为分partition的依据 每一个segment里面包含了两个文件 —— 索引文件、数据文件 ，segment文件以offset区间的起始值命名，长度固定20位，不足的位用0填充 例如存储了第0-20条的消息，segment文件就是：00000000000000000000.index00000000000000000000.log index文件结构很简单，每一行都是一个key,value对 key 是消息的序号offsetvalue 是消息的物理位置偏移量如 1,03,2996,497... log文件中保存了消息的实际内容，和相关信息 如消息的offset、消息的大小、消息校验码、消息数据等 2、根据offset检索数据流程 offset -> find segment -> look for index find 物理位置偏移量 -> read data from log file 四、Kafka实现高吞吐量 Kafka将所有的消息都写入磁盘存储，更加稳定、容量更大，更容易扩容。下面从几个方面解析kafka高吞吐量的原因 1、磁盘顺序读写 kafka的消息是最佳的方式写入的，这使得Kafka可以充分利用磁盘的顺序读写性能。顺序读写不需要硬盘磁头的寻道时间，只需很少的扇区旋转时间，所以速度远快于随机读写。Kafka官方给出了测试数据(Raid-5，7200rpm)：顺序 I/O: 600MB/s随机 I/O: 100KB/s 2、零拷贝 升级为 在Linux kernel2.2 之后出现了一种叫做\"零拷贝(zero-copy)\"系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”。系统上下文切换减少为2次，可以提升一倍的性能 3、文件分区 kafka的每个topi分成很多个partition，每个partition分为多个段segment，通过分片、分段的形式增加了并行度 4、batch机制 kafka除了可以单条数据处理，还可以批处理，减少服务端的I/O次数 5、数据压缩 Producer可以通过GZIP或Snappy格式对消息集合进行压缩，Consumer进行解压。压缩的好处就是减少传输的数据量，减轻对网络传输的压力。虽然增加了CPU的工作，但在对大数据处理上，瓶颈在网络上而不是CPU，所以这个成本很值得 五、Kafka高可用设计（HA） Kafka从0.8开始支持Partition的Replication，Kafka数据的读写只和Leader交互，其他follower从leader复制消息。 1、半同步、半异步 Kafka维护了一个ISR列表，在列表里的followers是完全同步（每次数据写入的时候必须所有的followers复制完成才返回成功），不在ISR列表内的followers是完全异步处理（只要leader写入成功就返回结束） 2、容灾 当leader挂掉，优先从ISR列表中选取leader。如果ISR列表中副本也都挂掉了，从剩下的follower种选一个作为leader，这时候不能确定数据同步是否执行完毕，所以会存在数据丢失的风险如果所有的副本都挂掉了，优先等待ISR副本恢复（最稳） 或者 只要有副本恢复就选举为leader（最快）可以通过配置文件配置 五、Kafka通讯 这个图采用的是SEDA多线程模型1、对于broker来说，客户端连接数量有限，不会频繁新建大量连接。因此一个Acceptor thread线程处理新建连接绰绰有余2、Kafka高吐吞量，则要求broker接收和发送数据必须快速，因此用proccssor thread线程池处理，并把读取客户端数据转交给缓冲区，不会导致客户端请求大量堆积3、Kafka磁盘操作比较频繁会且有io阻塞或等待，IO Thread线程数量一般设置为proccssor thread num两倍，可以根据运行环境需要进行调节 问题 1、kafka如何选举leader 2、同一个consumer group可能消费到重复的数据么 ​ consumer自己记录自当前消费的每个partition的offset，partition是分配给每个group内消费者的最小单元，所以同一个Group内的消费者共同消费一个topic，不会有数据重复 参考文章kafka官网Spark 实战, 第 2 部分:使用 Kafka 和 Spark Streaming 构建实时数据处理系统Kafka是如何实现高吞吐率的Kafka 高可用设计Kafka 消息存储及检索跟我学Kafka之NIO通信机制 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 17:00:41 "},"flume/flume_quick_start.html":{"url":"flume/flume_quick_start.html","title":"Flume Quick Start","keywords":"","body":"Flume Quick Start 一、开始 官网 User Guide Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。 Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，对数据进行简单处理，并写到各种数据接受方。 Flume有两个版本 Flume 0.9X版本的统称Flume-og多Master，为了保证配置数据的一致性，引入了ZooKeeper保存配置数据，ZooKeeper本身可保证配置数据的一致性和高可用，另外，在配置数据发生变化时，ZooKeeper可以通知Flume Master节点。Flume Master间使用gossip协议同步数据 Flume1.X版本的统称Flume-ng取消了集中管理配置的 Master 和 Zookeeper，变为一个纯粹的传输工具。读入数据和写出数据现在由不同的工作线程处理（称为 Runner）。 在 Flume-og 中，读入线程同样做写出工作（除了故障重试）。如果写出慢的话（不是完全失败），它将阻塞 Flume 接收数据的能力。这种异步的设计使读入线程可以顺畅的工作而无需关注下游的任何问题 二、安装配置 1、get 压缩包 wget http://mirrors.sonic.net/apache/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz 2、压缩文件解压 tar -zxf apache-flume-*-bin.tar.gz -C /usr/local/opt/flume/ 3、配置flume环境 vim /etc/profile 或者 vim ~/.bash_profile[MAC] ## flume configuration export FLUME_HOME=/usr/local/opt/flume/apache-flume-1.7.0-bin export PATH=.:$PATH::$FLUME_HOME/bin source /etc/profile 或者 source ~/.bash_profile[MAC] # 添加Java 环境变量 cp -f $FLUME_HOME/conf/flume-env.sh.template $FLUME_HOME/conf/flume-env.sh echo 'JAVA_HOME=/opt/jdk1.7.0_75/' >> $FLUME_HOME/conf/flume-env.sh echo 'confgratulations! fluem has been installed and flume-env.sh has been set!' 4、 测试是否安装配置成功 flume-ng version 三、Flume设计图解 系统结构图： 系统可分为三层 —— Agent、Collector、Store ——收集log并存起来每个flume可分三层 —— Source、Channel、Sink —— 收集/传递/存储 结构图详解： 单个 多个 复杂型 （三）Flume设计原理详解 每个flume分三层 —— Source、Channel、Sink —— 收集/传递/存储 Source有几种 Exec source实现：Unix command获得数据，最常用的就是tail -F [file]优点：简单，文件实时传输缺点：断点不能续传 Spooling Directory Source介绍：监控配置目录下新增的文件，并实时的将文件数据读出来优点：只要是在监控目录下的新文件都可以传输缺点：新文件不能再编辑、不能有子目录、只能做到近乎实时（如，一分钟一个文件） Channel有几种 MemoryChannel 可以实现高速的吞吐，但是无法保证数据的完整性 JDBC Channel MemoryRecoverChannel FileChannel - 官方推荐 保证数据的完整性与一致性 Sink 文件系统 集群文件系统 数据库 四、实践出新知 测试1 —— netcat_to_logger vim flume-netcat-to-logger.conf # Name the components on this agent agent_1.sources = source_1 agent_1.channels = channel_1 agent_1.sinks = sink_1 # Describe/configure the source agent_1.sources.source_1.type = netcat agent_1.sources.source_1.bind = localhost agent_1.sources.source_1.port = 44444 # Use a channel which buffers events in memory agent_1.channels.channel_1.type = memory agent_1.channels.channel_1.capacity = 1000 agent_1.channels.channel_1.transactionCapacity = 100 # Describe the sink agent_1.sinks.sink_1.type = logger # Bind the source and sink to the channel agent_1.sources.source_1.channels = channel_1 agent_1.sinks.sink_1.channel = channel_1 启动 bin/flume-ng agent --conf conf --conf-file conf/flume-netcat-to-logger.conf --name agent_1 -Dflume.root.logger=INFO,console 开启输入 telnet localhost 44444 测试2 —— file_to_kafka #编辑配置文件 vim conf/flume-file-to-kafka.conf a1.sources = source_1 a1.sinks = sink_1 a1.channels = channel_1 a1.sources.source_1.type = exec a1.sources.source_1.channels = channel_1 a1.sources.source_1.command = tail -F /tmp/flume/local/test.txt a1.channels.channel_1.type = memory a1.channels.channel_1.capacity = 1000 a1.channels.channel_1.transactionCapacity = 100 a1.sinks.sink_1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.sink_1.topic = test a1.sinks.sink_1.brokerList = localhost:9092 a1.sinks.sink_1.requiredAcks = 1 a1.sinks.sink_1.batchSize = 20 a1.sinks.sink_1.serializer.class=kafka.serializer.StringEncoder # Bind the source and sink to the channel a1.sources.source_1.channels = channel_1 a1.sinks.sink_1.channel = channel_1 ## 启动 bin/flume-ng agent --conf conf --conf-file conf/flume-file-to-kafka.conf -name a1 -Dflume.root.logger=INFO,console ## 测试脚本 for i in `seq 1 100` do echo $i done sh -x tmp.sh >> /tmp/flume/local/test.txt 测试2 —— kafka_to_HDFS ## 编辑配置文件 vim conf/flume-kafka-to-hdfs.conf agent1.sources = source_1 agent1.channels = channel_1 agent1.sinks = sink_1 agent1.sources.source_1.type = org.apache.flume.source.kafka.KafkaSource agent1.sources.source_1.zookeeperConnect = localhost:2181 agent1.sources.source_1.topic = test agent1.sources.source_1.groupId = flume-test agent1.sources.source_1.channels = channel_1 agent1.sources.source_1.interceptors = i1 agent1.sources.source_1.interceptors.i1.type = timestamp agent1.sources.source_1.kafka.consumer.timeout.ms = 100 agent1.channels.channel_1.type = memory agent1.channels.channel_1.capacity = 10000 agent1.channels.channel_1.transactionCapacity = 1000 agent1.sinks.sink_1.type = hdfs agent1.sinks.sink_1.hdfs.path = /flume/kafka/%{topic}/%y-%m-%d agent1.sinks.sink_1.hdfs.rollInterval = 5 agent1.sinks.sink_1.hdfs.rollSize = 0 agent1.sinks.sink_1.hdfs.rollCount = 0 agent1.sinks.sink_1.hdfs.fileType = DataStream agent1.sinks.sink_1.channel = channel_1 ## 启动服务 bin/flume-ng agent --conf conf --conf-file conf/flume-kafka-to-hdfs.conf -name agent1 -Dflume.root.logger=INFO,console ## 测试数据 for i in `seq 1 100` do echo $i done sh -x tmp.sh | /usr/local/opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 测试4 —— file_to_HDFS vim flume-hdfs.conf #define source monitor a file agent1.sources.source_1.type = exec agent1.sources.source_1.shell = /bin/bash -c agent1.sources.source_1.command = tail -n +0 -F /tmp/flume/id.txt agent1.sources.source_1.channels = channel_1 agent1.sources.source_1.threads = 5 # Define a memory channel called channel_1 on agent1 agent1.channels.channel_1.type = memory agent1.channels.channel_1.capacity = 100000 agent1.channels.channel_1.transactionCapacity = 100000 agent1.channels.channel_1.keep-alive = 30 # Define a logger sink that simply logs all events it receives # and connect it to the other end of the same channel. agent1.sinks.sink_1.channel = channel_1 agent1.sinks.sink_1.type = hdfs agent1.sinks.sink_1.hdfs.path = hdfs:///flume agent1.sinks.sink_1.hdfs.writeFormat = Text agent1.sinks.sink_1.hdfs.fileType = DataStream agent1.sinks.sink_1.hdfs.rollInterval = 0 agent1.sinks.sink_1.hdfs.rollSize = 1000000 agent1.sinks.sink_1.hdfs.rollCount = 0 agent1.sinks.sink_1.hdfs.batchSize = 1000 agent1.sinks.sink_1.hdfs.txnEventMax = 1000 agent1.sinks.sink_1.hdfs.callTimeout = 60000 agent1.sinks.sink_1.hdfs.appendTimeout = 60000 # Finally, now that we've defined all of our components, tell # agent1 which ones we want to activate. agent1.channels = channel_1 agent1.sources = source_1 agent1.sinks = sink_1 启动 ./flume-ng agent --conf ../conf/ -f ../conf/flume-hdfs.conf -n agent1 -Dflume.root.logger=INFO,console 【注】每一修改id.txt文件都会将全部的文件内容获取一次 测试5 —— kafka_to_kafka agent.sources = s1 agent.channels = c1 agent.sinks = k1 # singapore-cluster agent.sources.s1.type = org.apache.flume.source.kafka.KafkaSource agent.sources.s1.zookeeperConnect = 172.31.4.53:2181,172.31.4.54:2181,172.31.4.55:2181 #agent.sources.s1.zookeeperConnect = 10.10.20.14:2181,10.10.20.15:2181,10.10.20.16:2181 agent.sources.s1.topic = report-zhizi #agent.sources.s1.topic = testtopic agent.sources.s1.groupId = Jason-report-zhizi-2017022021 agent.sources.s1.kafka.consumer.timeout.ms = 10000 agent.sources.s1.batchSize = 20 agent.sources.s1.kafka.fetch.message.max.bytes = 2097152 agent.sources.s1.kafka.num.consumer.fetchers = 2 agent.sources.s1.kafka.auto.commit.enable= true agent.sources.s1.kafka.auto.offset.reset = largest agent.sources.s1.channels = c1 agent.sources.s1.interceptors = i1 agent.sources.s1.interceptors.i1.type = static agent.sources.s1.interceptors.i1.key = topic agent.sources.s1.interceptors.i1.preserveExisting = false agent.sources.s1.interceptors.i1.value = testtopic # Channels agent.channels.c1.type = memory agent.channels.c1.keep-alive = 3 agent.channels.c1.capacity = 20000 agent.channels.c1.transactionCapacity = 2000 agent.channels.c1.byteCapacityBufferPercentage = 20 agent.channels.c1.byteCapacity = 4000000000 agent.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink agent.sinks.k1.topic = testtopic agent.sinks.k1.brokerList = 10.10.20.14:9092,10.10.20.15:9092,10.10.20.16:9092 agent.sinks.k1.requiredAcks = 1 agent.sinks.k1.batchSize = 20 agent.sinks.k1.channel = c1 @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-12-01 16:36:53 "},"zookeeper/zookeeper_quick_start.html":{"url":"zookeeper/zookeeper_quick_start.html","title":"Zookeeper Quick Start","keywords":"","body":"Zookeeper Quick Start What is it? ZooKeeper是Google的Chubby一个开源的实现，是一个开源的，分布式的应用程序协调服务 是Hadoop和Hbase的重要组件 它是一个为分布式应用提供一致性服务的软件 提供的功能包括：配置维护、域名服务、分布式同步、组服务等 ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 简介： Zookeeper是有2n+1个节点（允许n个节点失效)组成的集群服务。 在Zookeeper服务有两个角色，一个是leader，负责写服务和数据同步；剩下的是follower，提供读服务。 leader失效后会在follower中重新选举新的leader。(paxos算法) 每个follower都和leader有连接，接受leader的数据更新操作。（zab算法） 客户端可以连接到每个server，每个server的数据完全相同。 Server记录事务日志和快照到持久存储。 Zookeeper特点 最终一致性：为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能 可靠性：如果消息被到一台服务器接受，那么它将被所有的服务器接受 实时性：Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口 独立性 ：各个Client之间互不干预 原子性 ：更新只能成功或者失败，没有中间状态 顺序性 ：所有Server，同一消息发布顺序一致 Zookeeper中角色 领导者(Leader)：领导者负责进行投票的发起和决议，更新系统状态，处理写请求 跟随者(Follwer)：Follower用于接收客户端的读写请求并向客户端返回结果，在选主过程中参与投票 观察者（Observer）：观察者可以接收客户端的读写请求，并将写请求转发给Leader，但Observer节点不参与投票过程，只同步leader状态，Observer的目的是为了，扩展系统，提高读取速度。在3.3.0版本之后，引入Observer角色的原因： Zookeeper需保证高可用和强一致性； 为了支持更多的客户端，需要增加更多Server； Server增多，投票阶段延迟增大，影响性能； 权衡伸缩性和高吞吐率，引入Observer ； Observer不参与投票； Observers接受客户端的连接，并将写请求转发给leader节点； 加入更多Observer节点，提高伸缩性，同时不影响吞吐率。 客户端(Client)： 执行读写请求的发起方 Zookeeper数据模型 Zookeeper，内部是一个分层的文件系统目录树结构，每一个节点对应一个Znode。每个Znode维护着一个属性结构，它包含着版本号(dataVersion)，时间戳(ctime,mtime)等状态信息。ZooKeeper正是使用节点的这些特性来实现它的某些特定功能。每当Znode的数据改变时，他相应的版本号将会增加。每当客户端检索数据时，它将同时检索数据的版本号。并且如果一个客户端执行了某个节点的更新或删除操作，他也必须提供要被操作的数据版本号。如果所提供的数据版本号与实际不匹配，那么这个操作将会失败。 注：节点（znode）都可以存数据，可以有子节点 ；节点不支持重命名；数据大小不超过1MB（可配置） Znode介绍 Znode是客户端访问ZooKeeper的主要实体，它包含以下几个特征： （1）Watches 　　客户端可以在节点上设置watch(我们称之为监视器)。当节点状态发生改变时(数据的增、删、改)将会触发watch所对应的操作。当watch被触发时，ZooKeeper将会向客户端发送且仅发送一条通知，因为watch只能被触发一次。 （2）数据访问 　　ZooKeeper中的每个节点存储的数据要被原子性的操作。也就是说读操作将获取与节点相关的所有数据，写操作也将替换掉节点的所有数据。另外，每一个节点都拥有自己的ACL(访问控制列表)，这个列表规定了用户的权限，即限定了特定用户对目标节点可以执行的操作。 （3）节点类型 ZooKeeper中的节点有两种，分别为临时节点和永久节点。节点的类型在创建时即被确定，并且不能改变。 　　ZooKeeper的临时节点：该节点的生命周期依赖于创建它们的会话。一旦会话结束，临时节点将被自动删除，当然可以也可以手动删除。另外，需要注意是， ZooKeeper的临时节点不允许拥有子节点。 　　ZooKeeper的永久节点：该节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，他们才能被删除。 （4）顺序节点（唯一性的保证） 　　　当创建Znode的时候，用户可以请求在ZooKeeper的路径结尾添加一个递增的计数。这个计数对于此节点的父节点来说是唯一的，它的格式为”%10d”(10位数字，没有数值的数位用0补充，例如”0000000001”)。当计数值大于232-1时，计数器将溢出。 org.apache.zookeeper.CreateMode中定义了四种节点类型，分别对应： PERSISTENT：永久节点 EPHEMERAL：临时节点 PERSISTENT_SEQUENTIAL：永久节点、序列化 EPHEMERAL_SEQUENTIAL：临时节点、序列化 安装 服务器环境：三台虚拟机，并且配置ssh互信： centos1 192.168.137.122； centos2 192.168.137.101； centos3 192.168.137.102 1、下载、解压ZooKeeper：wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz 解压到/usr/local/目录中： tar -xvzf zookeeper-3.4.6.tar.gz -C /usr/local/ 添加到环境变量： ZOOKEEPER_HOME=/usr/local/zookeeper-3.4.6/ export PATH=$PATH:$ZOOKEEPER_HOME/bin:$ZOOKEEPER_HOME/conf source /etc/profile 2、修改配置文件： 在zookeeper的conf/目录下，将zoo_sample.cfg重命名为zoo.cfg，然后： 将工作目录改为dataDir安装目录的data下（dataDir手工建立）：dataDir=/usr/local/zookeeper-3.4.6/dataDir 添加集群中的节点： server.1=centos1:2888:3888 server.2=centos2:2888:3888 server.3=centos3:2888:3888 在dataDir目录下创建接myid文件：根据server.X的号码在相应的节点上的dataDir下建立myid文件，内容为X 3、以上在其中一台机子配置好了zookeeper，然后将zookeeper的目录scp到另外的两台机子上即可：scp -r zookeeper-3.4.6 root@centos3:/usr/local/ 在另外两台机子上修改myid文件内容； 在另外两台机子上配置环境变量； 4、启动zookeeper集群：在三台机子上分别执行：zkServer.sh start 查看集群状态：zkServer.sh status 5、zookeeper配置文件说明： tickTime=2000：ZK中的一个时间单元。ZK中所有时间都是以这个时间单元为基础，进行整数倍配置的。例如，session的最小超时时间是2*tickTime initLimit=10：Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。Leader允许F在initLimit时间内完成这个工作。通常情况下，我们不用太在意这个参数的设置。如果ZK集群的数据量确实很大了，F在启动的时候，从Leader上同步数据的时间也会相应变长，因此在这种情况下，有必要适当调大这个参数了。 syncLimit=5：在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。如果L发出心跳包在syncLimit之后，还没有从F那里收到响应，那么就认为这个F已经不在线了。注意：不要把这个参数设置得过大，否则可能会掩盖一些问题。 clientPort=2181：客户端连接server的端口，即对外服务端口，一般设置为2181。 server.1=centos1:2888:3888：2888端口号是zookeeper服务之间通信的端口，而3888是zookeeper与其他应用程序通信的端口。而zookeeper是在hosts中已映射了本机的ip。 命令行操作 1、连接到zookeeper服务：# zkCli.sh -server centos3:2181 连接到了centos3上的zookeeper服务，其中2181是端口号； 或者 # zkCli.sh 连接到本机的zookeeper服务； 2、连接到zk服务后，在命令行输入man或help命令，可以常看zk的命令：[zk: localhost:2181(CONNECTED) 0] man ZooKeeper -server host:port cmd args connect host:port get path [watch] ls path [watch] set path data [version] rmr path delquota [-n|-b] path quit printwatches on|off create [-s] [-e] path data acl stat path [watch] close ls2 path [watch] history listquota path setAcl path acl getAcl path sync path redo cmdno addauth scheme auth delete path [version] setquota -n|-b val path 3、查看、创建、删除操作：#ZooKeeper的结构，很像是目录结构，ls一下，看到了一个默认的节点zookeeper [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] #创建一个新的节点，/node, 值是helloword [zk: localhost:2181(CONNECTED) 6] create /node helloworld Created /node #再看一下，恩，多了一个我们新建的节点/node,和zookeeper都是在/根目录下的。 [zk: localhost:2181(CONNECTED) 7] ls / [node, zookeeper] #看看节点的值是啥？还真是我们设置的helloword,还显示了创建时间，修改时间，version,长度，children个数等 [zk: localhost:2181(CONNECTED) 8] get /node helloworld cZxid = 0x200000003 ctime = Sun Feb 21 16:23:53 CST 2016 mZxid = 0x200000003 mtime = Sun Feb 21 16:23:53 CST 2016 pZxid = 0x200000003 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 10 numChildren = 0 #修改值，看看，创时间没变，修改时间变了，长度变了，数据版本值变了 [zk: localhost:2181(CONNECTED) 9] set /node helloworld! cZxid = 0x200000003 ctime = Sun Feb 21 16:23:53 CST 2016 mZxid = 0x200000004 mtime = Sun Feb 21 16:26:11 CST 2016 pZxid = 0x200000003 cversion = 0 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 0 #创建子节点 [zk: localhost:2181(CONNECTED) 10] create /node/node1 node1 Created /node/node1 #查看子节点 [zk: localhost:2181(CONNECTED) 11] ls /node [node1] #查看父节点，其中numChildren 改变了 [zk: localhost:2181(CONNECTED) 12] get /node helloworld! cZxid = 0x200000003 ctime = Sun Feb 21 16:23:53 CST 2016 mZxid = 0x200000004 mtime = Sun Feb 21 16:26:11 CST 2016 pZxid = 0x200000005 cversion = 1 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 1 #删除节点 [zk: localhost:2181(CONNECTED) 15] delete /test java 客户端 1、pom.xml org.apache.zookeeper zookeeper 3.4.6 2、代码： private ZooKeeper zk = null; /** * 创建ZK连接 * @param connectString * @param sessionTimeout */ public void createConnection( String connectString, int sessionTimeout ) { this.releaseConnection(); try { zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() { // 监控所有被触发的事件 public void process(WatchedEvent event) { // TODO Auto-generated method stub System.out.println(\"已经触发了\" + event.getType() + \"事件！\"); } }); } catch ( IOException e ) { System.out.println( \"连接创建失败，发生 IOException\" ); e.printStackTrace(); } } /** * 关闭ZK连接 */ public void releaseConnection() { if ( zk != null ) { try { this.zk.close(); } catch ( InterruptedException e ) { // ignore e.printStackTrace(); } } } /** * 创建节点 * @param path 节点path * @param data 初始数据内容 * @return */ public boolean createPath( String path, String data ) { try { String node = zk.create( path, data.getBytes(),Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT );//临时、永久...节点 System.out.println( \"节点创建成功, Path: \" + node + \", content: \" + data ); } catch ( KeeperException e ) { System.out.println( \"节点创建失败，发生KeeperException\" ); e.printStackTrace(); } catch ( InterruptedException e ) { System.out.println( \"节点创建失败，发生 InterruptedException\" ); e.printStackTrace(); } return true; } /** * 读取指定节点数据内容 * @param path 节点path * @return */ public String readData( String path ) { try { String res = new String( zk.getData( path, false, null ) ); System.out.println( \"获取数据成功：\" + res ); return res; } catch ( KeeperException e ) { System.out.println( \"读取数据失败，发生KeeperException，path: \" + path ); e.printStackTrace(); return \"\"; } catch ( InterruptedException e ) { System.out.println( \"读取数据失败，发生 InterruptedException，path: \" + path ); e.printStackTrace(); return \"\"; } } /** * 更新指定节点数据内容 * @param path 节点path * @param data 数据内容 * @return */ public boolean writeData( String path, String data ) { try { Stat stat = zk.setData( path, data.getBytes(), -1 );//-1表示匹配所有版本 System.out.println( \"更新数据成功，path：\" + path + \", stat: \" + stat); } catch ( KeeperException e ) { System.out.println( \"更新数据失败，发生KeeperException，path: \" + path ); e.printStackTrace(); } catch ( InterruptedException e ) { System.out.println( \"更新数据失败，发生 InterruptedException，path: \" + path ); e.printStackTrace(); } return false; } /** * 删除指定节点 * @param path 节点path */ public void deleteNode( String path ) { try { zk.delete( path, -1 ); System.out.println( \"删除节点成功，path：\" + path ); } catch ( KeeperException e ) { System.out.println( \"删除节点失败，发生KeeperException，path: \" + path ); e.printStackTrace(); } catch ( InterruptedException e ) { System.out.println( \"删除节点失败，发生 InterruptedException，path: \" + path ); e.printStackTrace(); } } public List getChildrens( String path ) { try { return zk.getChildren(path, true); } catch ( KeeperException e ) { System.out.println( \"删除节点失败，发生KeeperException，path: \" + path ); e.printStackTrace(); return null; } catch ( InterruptedException e ) { System.out.println( \"删除节点失败，发生 InterruptedException，path: \" + path ); e.printStackTrace(); return null; } } /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception { ZkTest test = new ZkTest(); test.createConnection(\"192.168.137.122:2181\", 3000); test.createPath(\"/node/node2\", \"node2\"); test.readData(\"/node\"); List childrens = test.getChildrens(\"/node\"); for (String str :childrens) { System.out.println(str); } test.releaseConnection(); } #zookeeper @ 学必求其心得，业必贵其专精 @ WHAT - HOW - WHY @ 不积跬步 - 无以至千里 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2019-11-28 10:06:33 "},"thanks.html":{"url":"thanks.html","title":"感谢&参考文章","keywords":"","body":"感谢&参考文章 Copyright (c) 2019 Tencent PRUCE & KB. all right reserved，powered by Gitbooktime： 2018-09-21 11:43:12 "}}